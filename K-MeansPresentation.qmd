---
title: "K-Means Presentation"
format: 
  revealjs:
    incremental: true
embed-resources: true
---
```{r}
load("df_norm.RData")
```
## An Introduction to K-means clustering {.scrollable .smaller}

::: incremental
Finding meaningful patterns within data has become obtrusive as data collection and management continues to grow at an unprecedented rate.

K-means clustering caters to such highly voluminous & unlabeled data. 

We will employ the k-means clustering algorithm to gain insights on customer segmentation in eCommerce data for a retail store based in the UK.
:::

--- 

By the end of this presentation we will have discussed the following concepts of k-means clustering:

::: {.fragment .fade-up}
-   Unsupervised learning
-   Clustering
-   Mathematical foundation of the k means algorithm by the Euclidean Distance.
:::

## Data Preparation {.scrollable .smaller}

- Data from online retailer based in UK
- 406,829 observations of customer transactions

::: fragment
::: panel-tabset

### Raw Data {.smaller}

![Raw Customer Data](rawdata.png)

### Tab 2

![Second tab of columns](data2.png)
:::
:::

::: notes
Quantity and UnitPrice are multiplied together to create total We want to exclude returns to avoid negative values for Quantity. Sales is the sum total derived from the new total column Orders are calculated by Invoice date to account for all transactions. Average sale is the difference of Sales and Orders.
:::

## Cleaned Data {.scrollable .smaller}
-   Nulls removed
-   Subset of data without UK sales.
-   Removed quantities $<0$
-   Created variables $Sales$, $Orders$ and $AvgSales$ to capture customer spending habits
![Data by Customer acquisition](customerdata.png)

## Data Distribution {.scrollable .smaller}

::: panel-tabset

### Top 10

![Data favors UK](countriesuk.png)

### Top 10 '-UK'

![Data favors UK](countries.png)

:::

## Distribution by CustomerID {.scrollable .smaller}

::: panel-tabset

### page 1
![Data distribution by CustomerID](customerdistr.png)

### page 2
![Data spread for Orders](Top5OrderCount.png)
:::

## An Introduction to K-means clustering {.scrollable .smaller}

::: nonincremental
There are 5 main steps to execute the k-means clustering method.
:::

::: { .fragent .fade-up}
-   Assign all data points to a cluster noted as $k$.
-   Set $k$ random seed points.
-   Reassign all data points to a cluster using distance to seed point.
-   Compute the mean center of each new cluster noted as a centroid.
-   Loop the re-assignment step of all data points to a new centroid based on its minimized relocation, until the centroid does not change.
:::

::: notes
We will discuss the 5 main steps of the method and apply it to an eCommerce data set but first it is necessary to introduce some components needed to set up the k-means clustering analysis.

K-means clustering will be used to classify consumer bases into discrete clusters based on shared characteristics. These clusters will be interpreted and discussed at the end of the analysis.
:::

## What is Unsupervised learning {.smaller}

::: incremental
-   Statistical modeling technique used to categorize/group data without the assistance of historical results or human intervention.
-   Autonomous data categorization
-   Creates a data driven outcome
:::

::: notes
The autonomous reassignment of data to a centroid is the act that makes this method a form of unsupervised learning.

This is rather a data driven outcome as the data fall into different clusters, new centroids are calculated based on those members.

Initial centroids are generated randomly in this generic model for K-means clustering, we will discuss how other methods intend to improve the model with a modified centroid selection.
:::

## Clustering {.smaller}

::: incremental
-   Clustering is the act of partitioning data into meaningful groups based on similarity in attributes.

-   The goal of clustering is to create insightful clusters to better understand connections in the data.
:::

## Clustering {.smaller}

::: columns
::: {.column width="50%"}
::: fragment
-   $k=4$, the seed point are green, forming 4 respective clusters
-   Once the initial assignment of centroids is made the Euclidean distance is used to establish cluster members by minimal distance noted as the red lines in the figure.
:::
:::
::: {.column width="50%"}
::: fragment
![centroids](centroids.png)
:::
:::
:::
::: notes
This example the data are clustered based on 4 randomly set seed centers, k=4.
:::

## Euclidean Distance {.smaller}

::: columns
::: {.column width="60%"}
::: fragment
- Euclidean distance is used to find dissimilarity between data points in order to group similar data instances into clusters 
- In k-means it is done so by calculating the distance between a data object and cluster center by:
:::

::: fragment
$$d(x,C_i)=sqrt(\sum_{i=1}^{N} (x_jâˆ’C_{ij})^2)$$
:::
:::

::: {.column width="40%"}
::: fragment
Objective Function:

- The k-means clustering objective is to minimize the within-cluster variance.

:::

::: fragment
It is formulated as:

$$ d(x,C_i)=(\sum_{i=1}^{k}*\sum_{x \in C_i}^{}(||x-\mu_i||)^2) $$
:::
:::
:::

## Euclidean Distance {.smaller}

::: nonincremental
-   $k$ is the number of clusters.

-   $C_i$ represents the number of points in the cluster $i$

-   $\mu_i$ represents the centroid mean of cluster $i$

-   In this context, similarity is inversely related to the Euclidean distance

-   The smaller the distance, the greater the similarity between objects
:::

## New Centroids {.smaller}

::: nonincremental
::: columns
::: {.column width="50%"}
-   K-means clustering reassigns the data points to each cluster based on the Euclidean Distance calculation.

-   A new centroid location is set by updating the position at each clusters mean center.
:::

::: {.column width="50%"}
![data points reassigned to centroids](clusters.png)
:::
:::
:::

## Determine K {.smaller}

::: incremental
-   The elbow method plots the within cluster sum of squares by $k$.
:::

::: fragment
```{r}
#| echo: fenced
# Create an empty vector to store WCSS values
wcss <- vector("numeric", length = 10)
# Iterate over a range of K values (e.g., from 1 to 10)
for (i in 1:10) {
  model <- kmeans(df_norm[c("Sales", "Orders", "AvgSale")], centers = i, nstart = 10)
  wcss[i] <- ceiling(model$tot.withinss)
}
```
:::

::: notes
-wcss notates the within cluster sums of squares total.

-While cluster numbers are selected before the analysis, the cluster location in the data is randomly generated as mentioned.
:::

## Elbow Method {.smaller}

::: panel-tabset

### Code

```{r, `code-line-numbers` "6-11"}
#| echo: true
#| fig-width: 10
#| fig-height: 4.5
library(ggplot2)
# Plot the WCSS values against the number of clusters
p1<- ggplot(data.frame(K=1:10, WCSS=wcss), aes(x=K, y=WCSS)) +
  geom_line() +
  geom_point() +
  labs(title="Elbow Method to Find Optimal K", x="Number of Clusters (K)", y="Within-Cluster-Sum-of-Squares (WCSS)") +
  scale_x_continuous(breaks = seq(0, 10, by = 1))
```


### Plot {.scrollable}

```{r}
p1
```

:::

::: notes
Based on the results of the Elbow Method, above, the total WCSS appears to decrease less and less starting at 4. This gives us the ideal starting value for the initial number of clusters in the K-means model.

To understand the model results we compare the results in the table for each cluster, considering what each value tell us about the customer interactions and how they effect the retailer.
:::

## Clustering Model & Results:

```{r}
#| echo: true
set.seed(100)
model1 <- kmeans(df_norm[c("Sales", "Orders", "AvgSale")],4)

```
![Cluster Results](results.png)

## Cluster Interpretation {.smaller}

Since the goal is to evaluate the clusters and find meaning in the results.
We can make connections with purchasing frequency and amount to marketing and pricing strategies to promote customer satisfaction.

- cluster 1: potentially newer customers, large target for marketing.

- cluster 2: target for marketing a select range of costlier items.

- cluster 3: likely frequent customers, may benefit from low to mid priced item recommendations to increase or maintain engagement.

- cluster 4: lowest investment return, lowest interactions, least concerns

::: notes

Here we have 4 distinct clusters all with different potential information about customer behavior.
:::

## Future Works {.smaller}

Over the past sixty years, improvements and additions have been made to the base k-means methodology in order to optimize performance and increase scalability

-   This data may benefit from the exploration of differences in distance formulas, Mahatten distance and Chebyshev distance.

-   Additional $k$ selection methods such as the Empirical and Silhouette methods, and the use of principal component analysis or other dimensionality reduction techniques to improve efficiency and overall clustering results.

