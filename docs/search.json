[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "K-Means Clustering And Its Place in Ecommerce",
    "section": "",
    "text": "Platforms for e-commerce gather gigabytes, terabytes, and even petabytes, of information about consumer interactions, product preferences, and transactional histories. There is a heightened demand for more robust analytical processes as we see data growing at an unprecedented rate. The use of unsupervised machine learning and efficient clustering algorithms is crucial to the marketing and e-commerce industry when it comes to delivering meaningful and accurate communications to consumers. If properly tapped into, this abundance of data has the potential to fundamentally alter how internet businesses personalize and enhance their services and growth strategies. In this setting, K-Means clustering, a cornerstone of data science, emerges as a guiding light, promising to uncover the hidden structures in e-commerce information and offer a deeper comprehension of consumer behavior. The volume of data will continue to grow as more people have access to an ever expanding internet for social media, online shopping, and even remote working opportunities, and so will the need for efficient and effective consumer marketing methods continue to grow.\n“Clustering has proven efficient in discovering subtle but tactical patterns or relationships buried within a repository of unlabeled data sets” (Ali and Kadhum 2017).The k-means clustering method takes care of the disadvantages seen in most other clustering classifications and has the ability to handle large amounts of data efficiently while considering many different attributes. Clustering techniques are mostly useful in exploratory data analysis and data mining however the k-means clustering is particularly known for its simplicity, and low computational complexity and seen as the most popular algorithm suitable in various applications.\nUnsupervised machine learning methods are powerful statistical modeling techniques that allow for data to be categorized or grouped without the assistance of historical results or human intervention. One of the longer-standing methods of autonomous data categorization comes in the form of the K-Means Clustering algorithm, and is generally selected for its simplicity and speed (Žalik 2008). The K-means method is considered unsupervised machine learning because the outcome is rather data driven , which is preferred as the formulation can be adjusted based on any changing dimensions in data.\nThe basic method of the k-means algorithm is to randomly identify a centroid for every cluster as k, and use it as a reference point to assign all data points that retain the greatest similarity to that center, creating a cluster. The Euclidean distance considers the maximum distance between cluster means and each data object; it is used to identify the cluster each object best belongs to. This is a unique application because the centroid is updated every iteration by calculating the new mean with those new cluster members, nearly perfecting the requirement for within cluster similarities. “The k-means clustering method is considered a variance minimization technique. It represents the data as a mixture of distributions (Gaussian, Poisson, etc.), where each distribution represents a sub-population (or cluster) of the data”(Morissette 2013). Our examination provides a thorough comprehension of its mathematical foundations and clustering concepts. We will show how K-Means may be used to classify consumer bases into discrete clusters based on shared characteristics in e-commerce data sets. Additionally, we will discuss what can be considered drawbacks of the k-means clustering method such as needing to predetermine the cluster number prior to analysis.\nK-Means Clustering helps organizations better target their marketing efforts and increase return on investment by defining client segments in the context of marketing. For space-efficient picture compression, image processing uses K-Means, which is important for web graphics and mobile applications. K-Means excels in anomaly detection in fraud detection, protecting financial systems and networks. Additionally, it is used by recommendation engines to customize the distribution of content, increasing user happiness and engagement. K-Means promotes patient profiling in the healthcare industry, advancing individualized treatments and medication development. Natural language processing uses its strength to group text data, improving content categorization and information retrieval. K-Means is a tool for geographic data analysis that provides regional understanding for urban planning and environmental studies.\nThis paper will explore how K-Means clustering is used in real-world e-commerce scenarios. This paper also highlights the practical advantages that e-commerce enterprises can gain from applying K-Means clustering techniques using real-world data.\nThis study examines K-Means clustering’s promise as well as the complexities and limitations of doing so on e-commerce data sets. Managing large and noisy data sets, guaranteeing robustness against outliers, and determining an appropriate value for the “K” parameter are a few of these difficulties that may need to be overcome. Our investigation will include methods for resolving these issues and enhancing the robustness and dependability of K-Means clustering as a tool for e-commerce analytics.\n\n\n\n\nAli, Huda H., and Lubna Emad Kadhum. 2017. “K- Means Clustering Algorithm Applications in Data Mining and Pattern Recognition.” In. https://api.semanticscholar.org/CorpusID:36213323.\n\n\nMorissette, Sylvain, Laurence AND Chartier. 2013. “The k-Means Clustering Technique: General Considerations and Implementation in Mathematica.” Tutorials in Quantitative Methods for Psychology 9 (1): 15–24. https://doi.org/10.20982/tqmp.09.1.p015.\n\n\nŽalik, Krista Rizman. 2008. “An Efficient k′-Means Clustering Algorithm.” Pattern Recognition Letters 29 (9): 1385–91. https://doi.org/https://doi.org/10.1016/j.patrec.2008.02.014."
  },
  {
    "objectID": "Methods.html",
    "href": "Methods.html",
    "title": "2  Overview",
    "section": "",
    "text": "The K-Means method, as described by Steinley, is designed to partition data that is \\(N\\) objects having measurements on \\(P\\) variables, into \\(K\\) classes (\\(C_1\\),\\(C_2\\),\\(C_3\\),…,\\(C_k\\)), where \\(C_k\\) is the set of \\(n_k\\) objects in cluster \\(k\\), where \\(K\\) is given (Steinley 2006). The method computes seed points, which are randomly assigned reference points called centroids. Taking note that a centroid is not considered a physically observed data point, but rather a location in the center of a three dimensional space that is used as a reference point for a cluster. The distance between the centroids and the surrounding data points is then calculated using a distance formula. Data points are measured and assigned to the closest centroid. Then the mean is derived for each newly created cluster which becomes the new centroid of each cluster. The process is then repeated until there are no changes in the distance and mean of data points to the assigned cluster centroids. While there are different distance calculations to choose from, like the Manhattan Distance or Chebyshev Distance, the most commonly used is the Euclidean Distance.\nThe k-means algorithm relies on Euclidean distance as a measure of similarity between data objects. Euclidean distance is a powerful tool for measuring similarity in various applications, including image processing, pattern recognition, and clustering, such as in the k-means algorithm, where it helps partition data into well-defined clusters based on their spatial relationships in the feature space. Euclidean distance can be employed as a key metric to measure the dissimilarity between data points, facilitating the grouping of similar data instances into clusters using the k-means clustering algorithm. This allows us to uncover patterns and relationships within the defined set of data, which is critical for achieving the research objectives.\nIn this context, similarity is inversely related to the Euclidean distance; in other words, the smaller the distance, the greater the similarity between objects. To initiate the algorithm, you must specify the initial number of clusters \\((K)\\) and their initial centers. The algorithm then iteratively adjusts these cluster centers based on the similarity between data objects and cluster centers. Clustering continues until the objective function converges, signaling the end of the process and yielding the final result.\n\n\n\n\nSteinley, Douglas. 2006. “K-Means Clustering: A Half-Century Synthesis.” British Journal of Mathematical & Statistical Psychology 59 (May): 1–34. https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2."
  },
  {
    "objectID": "Methods2.html",
    "href": "Methods2.html",
    "title": "3  Euclidean Distance & Finding K",
    "section": "",
    "text": "The formula for calculating the Euclidean distance between a data object and a cluster center can be expressed as follows:\n\\[\nd(x,C_i)=sqrt(\\sum_{i=1}^{N} (x_j−C_{ij})^2)\n\\]\nObjective Function: The K-means clustering objective is to minimize the within-cluster variance. It is formulated as:\n\\[\nd(x,C_i)=(\\sum_{i=1}^{k}*\\sum_{x \\in C_i}^{}(||x-\\mu_i||)^2)\n\\]\nWhere:\n\\(k\\) is the number of clusters.\n\\(C_i\\) represents the number of points in the cluster \\(i\\)\n\\(\\mu_i\\) represents the centroid mean of cluster \\(i\\)\nThe objective function is the sum of squared Euclidean distances from each point in a cluster to its centroid. The goal is to find cluster assignments and centroids that minimize this objective.\nTo determine k, the number of clusters to initially set, we can use the empirical method or the elbow method. The empirical method assigns the number of clusters \\(\\approx \\frac{n^{\\frac{1}{2}}}{2}\\) for a data set of \\(n\\) observations. The elbow method is based on the idea that increasing the number of clusters, \\(k\\), will reduce the sum of within-cluster variance. After plotting the sum of within-cluster variances as a function of the number of clusters \\(K\\), the inflection point is determined using the turning point in the curve."
  },
  {
    "objectID": "Methods3.html",
    "href": "Methods3.html",
    "title": "4  Step-By-Step",
    "section": "",
    "text": "The k-means algorithm for partitioning can be executed in 5 main steps.\n\nPartition \\(N\\) objects, or observations, into \\(K\\) nonempty subsets. Each cluster must not be empty and will have a different classification. This is \\(K\\), if \\(k=4\\) that means there will be four clusters.\nThen compute seed points, which are randomly assigned reference points called centroids. In Figure 4.1 \\(k=4\\) and are highlighted in green.\nAssign each object to the cluster with the nearest assigned centroid. Figure 4.1 shows the Euclidean distance calculation as a line connecting each observation to one of the 4 centroids.\n\n\n\n\nFigure 4.1: centroids\n\n\n\nAdjust the centroid location using the Euclidean distance formula to minimize the distance of each data observation to its assigned centroid. Figure 4.2 shows how the Euclidean distance drives some cluster members to reassign to a new cluster, influencing the centroids positions. This updates the cluster mean, \\(\\mu_i\\), in turn relocating the centroids from where they were seen in Figure 4.1.\nThe final step is to repeat steps 2-4 until there is no change in distance between the observations and the centroids.\n\n\n\n\nFigure 4.2: clusters\n\n\nThe result uncovers 4 distinct clusters containing the most similar observations within groups."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "5  Data, Preprocessing, & Modeling",
    "section": "",
    "text": "To begin, an appropriate data source is identified. The data used in this analysis comes from UC Irvine’s Machine Learning Repository, titled “Online Retail” (“Online Retail” 2015). In order to maintain a healthy set of usable data, disruptive null values from the the base data are removed and a subsequent view of data descriptions are explored.\n\n\nCode\ndata_nulls_removed <- na.omit(data)\nsummary(data_nulls_removed)\n\n\n  InvoiceNo          StockCode         Description           Quantity        \n Length:406829      Length:406829      Length:406829      Min.   :-80995.00  \n Class :character   Class :character   Class :character   1st Qu.:     2.00  \n Mode  :character   Mode  :character   Mode  :character   Median :     5.00  \n                                                          Mean   :    12.06  \n                                                          3rd Qu.:    12.00  \n                                                          Max.   : 80995.00  \n InvoiceDate          UnitPrice          CustomerID      Country         \n Length:406829      Min.   :    0.00   Min.   :12346   Length:406829     \n Class :character   1st Qu.:    1.25   1st Qu.:13953   Class :character  \n Mode  :character   Median :    1.95   Median :15152   Mode  :character  \n                    Mean   :    3.46   Mean   :15288                     \n                    3rd Qu.:    3.75   3rd Qu.:16791                     \n                    Max.   :38970.00   Max.   :18287                     \n\n\nThe descriptions show that the data, excluding observations with null values, consists of \\(406,829\\) total observations. This specific data set originates from an online retailer located in the United Kingdom, but provides retail service to many different countries and regions. From the summary of the chosen data set, \\(8\\) variables are identified: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country.\n\n\nCode\nhead(data_nulls_removed,10)\n\n\n\n\n  \n\n\n\nThe frequency of purchases by Country may help provide some context about the regional distribution of customers, as well as the possible variety of customers:\n\n\nCode\ntopCountries_all <- data_nulls_removed %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE)\n\nggplot(data = topCountries_all, mapping = aes(x=Country, y=n, fill=Country)) + \n  geom_bar(stat=\"identity\") +\n  labs(title = \"Top 10 Countries\", y=\"Total Lifetime Transactions\") +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\nIt quickly becomes apparent that an overwhelming amount of data comes from the UK, as expected, which could skew the results during any analysis. In order to avoid this, a subset of data is generated which excludes data associated with the UK.\n\n\nCode\ndata_no_uk <- data_nulls_removed[data_nulls_removed$Country != \"United Kingdom\",]\n\n\nWith the UK data removed, the top \\(10\\) frequencies are examined:\n\n\nCode\ntopCountries <- data_no_uk %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE) \n\nggplot(data = topCountries, mapping = aes(x=Country, y=n, fill=Country)) + \n  geom_bar(stat=\"identity\") +\n  labs(title = \"Top 10 Countries (excluding UK)\", y=\"Total Lifetime Transactions\") +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\nNow that the distribution of data does not primarily fall under a single country’s customer base, the analysis can proceed.\nReferring back to the view of the data table, there appears to be more than \\(1\\) observation per InvoiceNo. This means that the data is not grouped by any particular category, product, or location. In order to create a better view of each customer and their spending habits, some simple calculated fields are created and assigned back to the data in use.\nHere the Quantity and UnitPrice columns are multiplied together and assigned to a new variable called “total” in order to obtain the total cost based on the number of items purchased per observation:\n\n\nCode\ndata_no_uk <- data_no_uk %>% mutate(total = data_no_uk$Quantity*data_no_uk$UnitPrice)\n\n\nThe data is beginning to look usable and gives a more robust view of the customers, but there are still some improvements that can be made to ensure bias is avoided. Since the interest lies in purchasing-customer clustering, any customers or invoices that include returns or negative values for Quantity can be excluded from the data.\n\n\nCode\ndata_no_uk <- data_no_uk %>% subset(Quantity > 0)\n\n\nFor the remainder of the analysis, only the applicable columns of interest are retained and assigned to a new data frame called “df1”:\n\n\nCode\ndf1 <- data_no_uk %>% dplyr::select(Quantity,UnitPrice,CustomerID,InvoiceDate,total)\n\n\nNext, a sum total of sales from the previously derived column “total” is created as “Sales”, along with a unique count of InvoiceDate to provide the number of orders made called “Orders”, and the difference of those two variables called “AvgSale”. The data is then grouped by CustomerID to decrease the granularity and provide a more complete view of each customer and their purchasing history and habits. This new set of data will be labeled “customer_data”:\n\n\nCode\ncustomer_data <- df1 %>% group_by(CustomerID) %>% summarize(Sales=sum(total), Orders=length(unique(InvoiceDate))) %>% mutate(AvgSale=Sales/Orders)\n\n\n\n\n\n\n  \n\n\n\n\n\n\nBased on the newly compressed data, customers’ behaviors can be visualized and used in a meaningful clustering exercise:\n\n\n\n\n\n\nFigure 5.1: Top 5 Avg Sales\n\n\n\n\n\n\n\nFigure 5.2: Bottom 5 Avg Sales\n\n\n\n\n\n\n\n\n\nFigure 5.3: Top 5 Sales\n\n\n\n\n\n\n\nFigure 5.4: Bottom 5 Sales\n\n\n\n\n\n\n\n\n\nFigure 5.5: Top 5 Order Frequencies\n\n\n\n\n\nHaving a better view and understanding of the data format and structure, the next step is to normalize the data in order to produce a normal distribution across the different variables. This is done by ranking the Sales, Orders, and Average Sales. Once these values have been ranked, they can be scaled for the final output. This final output is called “df_norm”.\n\n\nCode\ndf_ranked <- customer_data %>% mutate(Sales=rank(Sales), Orders=rank(Orders, ties.method = \"first\"), AvgSale=rank(AvgSale))\n\ndf_norm <- df_ranked %>% mutate(Sales=scale(Sales), Orders=scale(Orders), AvgSale=scale(AvgSale))\n\n\nThe resulting data is verified as normal by checking the standard deviation of each variable. CustomerID will not be normalized since it is an identifier for each record:\n\n\nCustomerID      Sales     Orders    AvgSale \n  891.1709     1.0000     1.0000     1.0000 \n\n\nNow that the data is normalized, a value is determined for \\(k\\) using the Elbow Method:\n\n\nCode\n# Create an empty vector to store WCSS values\nwcss <- vector(\"numeric\", length = 10)\n\n# Iterate over a range of K values (e.g., from 1 to 10)\nfor (i in 1:10) {\n  model <- kmeans(df_norm[c(\"Sales\", \"Orders\", \"AvgSale\")], centers = i, nstart = 10)\n  wcss[i] <- ceiling(model$tot.withinss)\n}\n\n# Plot the WCSS values against the number of clusters\nggplot(data.frame(K=1:10, WCSS=wcss), aes(x=K, y=WCSS)) +\n  geom_line() +\n  geom_point() +\n  labs(title=\"Elbow Method to Find Optimal K\", x=\"Number of Clusters (K)\", y=\"Within-Cluster-Sum-of-Squares (WCSS)\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1))\n\n\n\n\n\nBased on the results of the Elbow Method, above, the total WCSS appears to decrease less and less starting at \\(4\\). This gives us the ideal starting value for the initial number of clusters in the k-means model.\nTo build the model, a seed is set (in order to make the results reproducible) and the model function is initialized:\n\n\nCode\nset.seed(100)\nmodel1 <- kmeans(df_norm[c(\"Sales\", \"Orders\", \"AvgSale\")],4)\n\n\nThe raw model results are presented here:\n\n\nK-means clustering with 4 clusters of sizes 75, 73, 145, 125\n\nCluster means:\n       Sales     Orders    AvgSale\n1 -0.6837708 -0.9636583  0.1547879\n2  0.5362754 -0.5003309  1.2053864\n3  0.9694582  1.0724979  0.3112028\n4 -1.0274938 -0.3737094 -1.1578137\n\nClustering vector:\n  [1] 3 3 2 1 3 4 2 1 2 2 2 3 2 4 3 4 3 4 4 3 2 3 1 1 4 2 2 1 3 3 3 4 4 3 1 1 3\n [38] 2 3 2 2 3 4 4 1 2 2 3 3 2 1 3 4 4 3 3 1 1 4 4 3 2 1 1 4 3 3 4 3 3 3 1 2 1\n [75] 3 2 4 4 3 4 2 1 1 3 4 3 4 1 2 3 2 3 1 1 2 3 3 1 1 3 3 3 3 1 3 3 1 1 3 3 3\n[112] 3 3 1 3 1 4 4 3 2 4 3 2 3 4 4 3 4 4 1 4 1 2 1 2 3 3 1 3 1 4 3 3 3 4 3 3 4\n[149] 1 2 1 2 1 3 3 4 1 1 2 4 4 2 4 4 3 4 3 4 4 2 4 3 1 4 1 3 3 4 1 4 4 3 4 2 1\n[186] 1 4 4 3 3 2 4 4 4 4 2 1 4 4 3 4 3 3 4 3 4 4 1 4 2 3 3 2 1 3 1 3 1 1 4 3 3\n[223] 3 4 4 4 2 3 3 4 2 1 3 2 3 1 1 3 4 3 3 1 3 2 3 4 4 4 4 2 2 4 3 3 1 4 4 3 3\n[260] 4 1 3 2 3 4 4 3 3 3 3 4 3 3 3 3 3 4 2 2 1 4 4 1 4 4 3 1 4 4 3 3 1 3 3 3 3\n[297] 2 3 1 2 2 4 4 3 3 4 4 4 1 3 3 4 1 3 4 1 4 1 4 4 2 1 4 3 4 2 3 3 2 4 3 2 3\n[334] 2 4 2 3 2 2 2 3 4 3 1 3 3 4 4 4 4 4 4 1 4 1 1 4 2 1 4 1 4 4 4 2 4 4 2 2 1\n[371] 2 3 4 2 3 3 4 3 4 4 3 2 3 4 4 4 3 3 4 2 4 2 2 2 3 3 1 3 3 1 2 2 3 3 3 3 4\n[408] 2 2 4 4 3 1 3 1 3 2 4\n\nWithin cluster sum of squares by cluster:\n[1]  42.17291  55.77070 147.56352 119.54895\n (between_SS / total_SS =  70.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nWhile the raw output is useful, the best tool is to visualize the observations based on their resulting clusters.\nPlot and view cluster results:\n\n\nCode\nfviz_cluster(model1, data = df_norm,\n             geom = \"point\",\n             ellipse.type = \"convex\",\n             ggtheme = theme_bw()\n)\n\n\n\n\n\nSince the model utilizes more than \\(2\\) variables, the axis of the graph are based on principle components instead of each variable.\n\n\nCode\ndf_norm$Cluster2 <- as.factor(model1$cluster)\n\nplot_ly(df_norm, x=~Orders, y=~Sales, z=~AvgSale, color=~Cluster2) %>% \n  add_markers(size=2)\n\n\n\n\n\n\n\nCluster Explanation\n\nCluster 1 - Customers in this cluster have lower total sales, low number of orders, and relatively average per-order sales. These customers could potentially be newer customers and would benefit from some targeted marketing in order to drive engagement and increase total sales.\nCluster 2 - The customers in this cluster have average total sales with slightly below average number of orders and very high average sales. These customers buy expensive items but not very frequently. The business would benefit from marketing their more expensive items to this customer group.\nCluster 3 - Customers in cluster three have higher total sales and number of orders, but average order value is not very high. They most likely purchase frequently and would benefit from low-mid priced item recommendations to increase or maintain these customers’ engagement.\nCluster 4 - These customers have the lowest total sales, lower number of orders, and low average sales amounts.\n\n\n\n\n\n\n“Online Retail.” 2015. UCI Machine Learning Repository."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ali, Huda H., and Lubna Emad Kadhum. 2017. “K- Means Clustering\nAlgorithm Applications in Data Mining and Pattern Recognition.”\nIn. https://api.semanticscholar.org/CorpusID:36213323.\n\n\nMorissette, Sylvain, Laurence AND Chartier. 2013. “The k-Means\nClustering Technique: General Considerations and Implementation in\nMathematica.” Tutorials in Quantitative Methods for\nPsychology 9 (1): 15–24. https://doi.org/10.20982/tqmp.09.1.p015.\n\n\n“Online Retail.” 2015. UCI Machine Learning\nRepository.\n\n\nSteinley, Douglas. 2006. “K-Means Clustering: A Half-Century\nSynthesis.” British Journal of Mathematical & Statistical\nPsychology 59 (May): 1–34. https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2.\n\n\nŽalik, Krista Rizman. 2008. “An Efficient k′-Means Clustering\nAlgorithm.” Pattern Recognition Letters 29 (9): 1385–91.\nhttps://doi.org/https://doi.org/10.1016/j.patrec.2008.02.014."
  }
]