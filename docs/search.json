[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "K-Means Clustering and Its Place In Ecommerce",
    "section": "",
    "text": "Introduction\nPlatforms for e-commerce gather gigabytes, terabytes, and even petabytes of information about consumer interactions, product preferences, and transactional histories. There is a heightened demand for more robust analytical processes as data continues growing at an unprecedented rate. The use of unsupervised machine learning and efficient clustering algorithms is crucial to the marketing and e-commerce industry when it comes to delivering meaningful and accurate communications to consumers. If properly tapped into, this abundance of data has the potential to fundamentally alter how internet businesses personalize and enhance their services and growth strategies. In this setting, k-means clustering, a cornerstone of data science, emerges as a guiding light, promising to uncover the hidden structures in e-commerce information and offer a deeper comprehension of consumer behavior. The volume of data will continue to grow as more people have access to an ever expanding internet for social media, online shopping, and even remote working opportunities, and so will the need for efficient and effective consumer marketing methods continue to grow.\n“Clustering has proven efficient in discovering subtle but tactical patterns or relationships buried within a repository of unlabeled data sets” (Ali and Kadhum 2017). The k-means clustering method takes care of the disadvantages seen in most other clustering classifications and has the ability to handle large amounts of data efficiently while considering many different attributes. Clustering techniques are mostly useful in exploratory data analysis and data mining however the k-means clustering is particularly known for its simplicity, and low computational complexity and seen as the most popular algorithm suitable in various applications.\nUnsupervised machine learning methods are powerful statistical modeling techniques that allow for data to be categorized or grouped without the assistance of historical results or human intervention. One of the longer-standing methods of autonomous data categorization comes in the form of the k-means clustering algorithm, and is generally selected for its simplicity and speed (Žalik 2008). The K-means method is considered unsupervised machine learning because the outcome is rather data driven , which is preferred as the formulation can be adjusted based on any changing dimensions in data.\nThe basic method of the k-means algorithm is to randomly identify a centroid for every cluster as k, and use it as a reference point to assign all data points that retain the greatest similarity to that center, creating a cluster. The Euclidean distance considers the maximum distance between cluster means and each data object; it is used to identify the cluster each object best belongs to. This is a unique application because the centroid is updated every iteration by calculating the new mean with those new cluster members, nearly perfecting the requirement for within cluster similarities. “The k-means clustering method is considered a variance minimization technique. It represents the data as a mixture of distributions (Gaussian, Poisson, etc.), where each distribution represents a sub-population (or cluster) of the data”(Morissette 2013). Our examination provides a thorough comprehension of its mathematical foundations and clustering concepts. This study will show how k-means may be used to classify consumer bases into discrete clusters based on shared characteristics in e-commerce data sets.\nK-means clustering helps organizations better target their marketing efforts and increase return on investment by defining client segments in the context of marketing. For space-efficient picture compression, image processing uses k-means, which is important for web graphics and mobile applications. K-means excels in anomaly detection in fraud detection, protecting financial systems and networks. Additionally, it is used by recommendation engines to customize the distribution of content, increasing user happiness and engagement. K-means promotes patient profiling in the healthcare industry, advancing individualized treatments and medication development. Natural language processing uses its strength to group text data, improving content categorization and information retrieval. K-means is a tool for geographic data analysis that provides regional understanding for urban planning and environmental studies.\nThis objective of the study is to explore how k-means clustering is used in a real-world e-commerce scenario. It will also highlight the practical advantages that e-commerce businesses can gain from applying k-means clustering techniques using real data. This study examines k-means clustering’s promise as well as the complexities and limitations of doing so on e-commerce data. Managing large and noisy data sets, guaranteeing robustness against outliers, and determining an appropriate value for the “K” parameter are a few of the difficulties that are addressed in during the process.\n\n\n\n\nAli, Huda H., and Lubna Emad Kadhum. 2017. “K- Means Clustering Algorithm Applications in Data Mining and Pattern Recognition.” In. https://api.semanticscholar.org/CorpusID:36213323.\n\n\nMorissette, Sylvain, Laurence AND Chartier. 2013. “The k-Means Clustering Technique: General Considerations and Implementation in Mathematica.” Tutorials in Quantitative Methods for Psychology 9 (1): 15–24. https://doi.org/10.20982/tqmp.09.1.p015.\n\n\nŽalik, Krista Rizman. 2008. “An Efficient k′-Means Clustering Algorithm.” Pattern Recognition Letters 29 (9): 1385–91. https://doi.org/https://doi.org/10.1016/j.patrec.2008.02.014."
  },
  {
    "objectID": "Methods.html",
    "href": "Methods.html",
    "title": "Overview",
    "section": "",
    "text": "The k-means method, as described by Steinley, is designed to partition data that is \\(N\\) objects having measurements on \\(P\\) variables, into \\(K\\) classes (\\(C_1\\),\\(C_2\\),\\(C_3\\),…,\\(C_k\\)), where \\(C_k\\) is the set of \\(n_k\\) objects in cluster \\(k\\), where \\(K\\) is given (Steinley 2006). The method computes seed points, which are randomly assigned reference points called centroids. Taking note that a centroid is not considered a physically observed data point, but rather a location in the center of a three dimensional space that is used as a reference point for a cluster. The distance between the centroids and the surrounding data points is then calculated using a distance formula. Data points are measured and assigned to the closest centroid. Then the mean is derived for each newly created cluster which becomes the new centroid of each cluster. The process is then repeated until there are no changes in the distance and mean of data points to the assigned cluster centroids. While there are different distance calculations to choose from, like the Manhattan Distance or Chebyshev Distance, the most commonly used is the Euclidean Distance.\nThe k-means algorithm relies on Euclidean distance as a measure of similarity between data objects. Euclidean distance is a powerful tool for measuring similarity in various applications, including image processing, pattern recognition, and clustering, such as in the k-means algorithm, where it helps partition data into well-defined clusters based on their spatial relationships in the feature space. Euclidean distance can be employed as a key metric to measure the dissimilarity between data points, facilitating the grouping of similar data instances into clusters using the k-means clustering algorithm. This allows us to uncover patterns and relationships within the defined set of data, which is critical for achieving the research objectives.\nIn this context, similarity is inversely related to the Euclidean distance; in other words, the smaller the distance, the greater the similarity between objects. To initiate the algorithm, an initial number of clusters \\((K)\\) and their initial centers are selected. The algorithm then iteratively adjusts these cluster centers based on the similarity between data objects and cluster centers. Clustering continues until the objective function converges, signaling the end of the process and yielding the final result.\n\n\n\n\nSteinley, Douglas. 2006. “K-Means Clustering: A Half-Century Synthesis.” British Journal of Mathematical & Statistical Psychology 59 (May): 1–34. https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2."
  },
  {
    "objectID": "Methods2.html",
    "href": "Methods2.html",
    "title": "Euclidean Distance & Finding K",
    "section": "",
    "text": "The formula for calculating the Euclidean distance between a data object and a cluster center can be expressed as follows:\n\\[\nd(x,C_i)=sqrt(\\sum_{i=1}^{N} (x_j−C_{ij})^2)\n\\]\nObjective Function: The k-means clustering objective is to minimize the within-cluster variance. It is formulated as:\n\\[\nd(x,C_i)=(\\sum_{i=1}^{k}*\\sum_{x \\in C_i}^{}(||x-\\mu_i||)^2)\n\\]\nWhere:\n\\(k\\) is the number of clusters.\n\\(C_i\\) represents the number of points in the cluster \\(i\\)\n\\(\\mu_i\\) represents the centroid mean of cluster \\(i\\)\nThe objective function is the sum of squared Euclidean distances from each point in a cluster to its centroid. The goal is to find cluster assignments and centroids that minimize this objective.\nTo determine \\(k\\), the number of clusters to initially set, there are a few approaches such as the empirical, elbow, and Silhouette methods. The empirical method assigns the number of clusters \\(\\approx \\frac{n^{\\frac{1}{2}}}{2}\\) for a data set of \\(n\\) observations. The elbow method is based on the idea that increasing the number of clusters, \\(k\\), will reduce the sum of within-cluster variance. After plotting the sum of within-cluster variances as a function of the number of clusters \\(K\\), the inflection point is determined using the turning point in the curve."
  },
  {
    "objectID": "Methods3.html",
    "href": "Methods3.html",
    "title": "Step by Step",
    "section": "",
    "text": "The k-means algorithm for partitioning can be executed in 5 main steps.\n\nPartition \\(N\\) objects, or observations, into \\(K\\) nonempty subsets. Each cluster must not be empty and will have a different classification. This is \\(K\\), if \\(k=4\\) that means there will be four clusters.\nThen compute seed points, which are randomly assigned reference points called centroids. In Figure 1 \\(k=4\\) and are highlighted in green.\nAssign each object to the cluster with the nearest assigned centroid. Figure 1 shows the Euclidean distance calculation as a line connecting each observation to one of the 4 centroids.\n\n\n\n\nFigure 1: centroids\n\n\n\nAdjust the centroid location using the Euclidean distance formula to minimize the distance of each data observation to its assigned centroid. Figure 2 shows how the Euclidean distance drives some cluster members to reassign to a new cluster, influencing the centroids positions. This updates the cluster mean, \\(\\mu_i\\), in turn relocating the centroids from where they were seen in Figure 1.\nThe final step is to repeat steps 2-4 until there is no change in distance between the observations and the centroids.\n\n\n\n\nFigure 2: clusters\n\n\nThe result uncovers 4 distinct clusters containing the most similar observations within groups."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "To begin, an appropriate data source is identified. The data used in this analysis comes from UC Irvine’s Machine Learning Repository, (“Online Retail” 2015). In order to maintain a healthy set of usable data, disruptive null values from the the base data are removed and a subsequent view of data descriptions are explored.\n\n\n\n\n\nCode\ndata_nulls_removed <- na.omit(data)\ndata_summary <- data.frame(InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country)\ndata_summary\n\n\n\n\n  \n\n\n\n\n\n\nThe descriptions show that the data, excluding observations with null values, consists of \\(406,829\\) total observations. This specific data set originates from an online retailer located in the United Kingdom, but provides retail service to many different countries and regions. From the summary of the chosen data set, \\(8\\) variables are identified: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country. The data types can be ignored in this particular description table, as they will be correct in the data view below:\n\n\nCode\nhead(data_nulls_removed,10)\n\n\n\n\n  \n\n\n\nThe frequency of purchases by Country may help provide some context about the regional distribution of customers, as well as the possible variety of customers:\n\n\nCode\ntopCountries_all <- data_nulls_removed %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE)\n\nggplot(data = topCountries_all, mapping = aes(x=Country, y=n, fill=Country)) + \n  geom_bar(stat=\"identity\") +\n  labs(title = \"Top 10 Countries\", y=\"Total Lifetime Transactions\") +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\nIt quickly becomes apparent that an overwhelming amount of data comes from the UK, as expected, which could skew the results during any analysis. In order to avoid this, a subset of data is generated which excludes data associated with the UK.\n\n\nCode\ndata_no_uk <- data_nulls_removed[data_nulls_removed$Country != \"United Kingdom\",]\n\n\nWith the UK data removed, the top \\(10\\) frequencies are examined:\n\n\nCode\ntopCountries <- data_no_uk %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE) \n\nggplot(data = topCountries, mapping = aes(x=Country, y=n, fill=Country)) + \n  geom_bar(stat=\"identity\") +\n  labs(title = \"Top 10 Countries (excluding UK)\", y=\"Total Lifetime Transactions\") +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\nNow that the distribution of data does not primarily fall under a single country’s customer base, the analysis can proceed.\nReferring back to the view of the data table, there appears to be more than \\(1\\) observation per InvoiceNo. This means that the data is not grouped by any particular category, product, or location. In order to create a better view of each customer and their spending habits, some simple calculated fields are created and assigned back to the data in use.\nHere the Quantity and UnitPrice columns are multiplied together and assigned to a new variable called “total” in order to obtain the total cost based on the number of items purchased per observation:\n\n\nCode\ndata_no_uk <- data_no_uk %>% mutate(total = data_no_uk$Quantity*data_no_uk$UnitPrice)\n\n\nThe data is beginning to look usable and gives a more robust view of the customers, but there are still some improvements that can be made to ensure bias is avoided. Since the interest lies in purchasing-customer clustering, any customers or invoices that include returns or negative values for Quantity can be excluded from the data.\n\n\nCode\ndata_no_uk <- data_no_uk %>% subset(Quantity > 0)\n\n\nFor the remainder of the analysis, only the applicable columns of interest are retained and assigned to a new data frame called “df1”:\n\n\nCode\ndf1 <- data_no_uk %>% dplyr::select(Quantity,UnitPrice,CustomerID,InvoiceDate,total)\n\n\nNext, a sum total of sales from the previously derived column “total” is created as “Sales”, along with a unique count of InvoiceDate to provide the number of orders made called “Orders”, and the difference of those two variables called “AvgSale”. The data is then grouped by CustomerID to decrease the granularity and provide a more complete view of each customer and their purchasing history and habits. This new set of data will be labeled “customer_data”:\n\n\nCode\ncustomer_data <- df1 %>% group_by(CustomerID) %>% summarize(Sales=sum(total), Orders=length(unique(InvoiceDate))) %>% mutate(AvgSale=Sales/Orders)\n\n\n\n\n\n\n  \n\n\n\n\n\n\nBased on the newly compressed data, customers’ behaviors can be visualized and used in a meaningful clustering exercise:\n\n\n\n\n\n\nFigure 1: Top 5 Avg Sales\n\n\n\n\n\n\n\nFigure 2: Bottom 5 Avg Sales\n\n\n\n\n\n\n\n\n\nFigure 3: Top 5 Sales\n\n\n\n\n\n\n\nFigure 4: Bottom 5 Sales\n\n\n\n\n\n\n\n\n\nFigure 5: Top 5 Order Frequencies\n\n\n\n\n\nHaving a better view and understanding of the data format and structure, the next step is to normalize the data in order to produce a normal distribution across the different variables. This is done by ranking the Sales, Orders, and Average Sales. Once these values have been ranked, they can be scaled for the final output. This final output is called “df_norm”.\n\n\nCode\ndf_ranked <- customer_data %>% mutate(Sales=rank(Sales), Orders=rank(Orders, ties.method = \"first\"), AvgSale=rank(AvgSale))\n\ndf_norm <- df_ranked %>% mutate(Sales=scale(Sales), Orders=scale(Orders), AvgSale=scale(AvgSale))\n\n\nThe resulting data is verified as normal by checking the standard deviation of each variable. CustomerID will not be normalized since it is an identifier for each record:\n\n\n\n\n  \n\n\n\nNow that the data is normalized, a value is determined for \\(k\\) using the Elbow Method:\n\n\nCode\n# Create an empty vector to store WCSS values\nwcss <- vector(\"numeric\", length = 10)\n\n# Iterate over a range of K values (e.g., from 1 to 10)\nfor (i in 1:10) {\n  model <- kmeans(df_norm[c(\"Sales\", \"Orders\", \"AvgSale\")], centers = i, nstart = 10)\n  wcss[i] <- ceiling(model$tot.withinss)\n}\n\n# Plot the WCSS values against the number of clusters\nggplot(data.frame(K=1:10, WCSS=wcss), aes(x=K, y=WCSS)) +\n  geom_line() +\n  geom_point() +\n  labs(title=\"Elbow Method to Find Optimal K\", x=\"Number of Clusters (K)\", y=\"Within-Cluster-Sum-of-Squares (WCSS)\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1))\n\n\n\n\n\nBased on the results of the Elbow Method, above, the total WCSS appears to decrease less and less starting at \\(4\\). This gives us the ideal starting value for the initial number of clusters in the k-means model.\nTo build the model, a seed is set (in order to make the results reproducible) and the model function is initialized:\n\n\nCode\nset.seed(100)\nmodel1 <- kmeans(df_norm[c(\"Sales\", \"Orders\", \"AvgSale\")],4)\n\n\nThe raw model results are presented here:\n\n\n\n\n  \n\n\n\nWhile the model output is useful, the best tool is to visualize the observations based on their resulting clusters.\nPlot and view cluster results:\n\n\nCode\nfviz_cluster(model1, data = df_norm,\n             geom = \"point\",\n             ellipse.type = \"convex\",\n             ggtheme = theme_bw()\n)\n\n\n\n\n\nSince the model utilizes more than \\(2\\) variables, the axis of the graph, above, are based on principle components instead of the individual model variables.\nThe graph below shows the customers in relation to all three model variables.\n\n\n\n\n\nCode\nplot_ly(customer_data, x=~Orders, y=~Sales, z=~AvgSale,\n        color=~Cluster2,\n        colors=c(\"red\",\"green\",\"blue\",\"violet\")) %>% \n  add_markers(size=2) %>% \n  layout(scene = list(xaxis=axx,yaxis=axy,zaxis=axz))\n\n\n\n\nInteractive graph (left click to rotate / right click to move)\n\n\n\n\n\n\n“Online Retail.” 2015. UCI Machine Learning Repository."
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Cluster Explanation\n\nCluster 1 - Customers in this cluster have lower total sales, a low number of orders, and relatively average per-order sales. These customers could potentially be newer customers and would benefit from some targeted marketing in order to drive engagement and increase total sales.\nCluster 2 - The customers in this cluster have average total sales with slightly below average number of orders and very high per-order sales. These customers buy expensive items but not very frequently. The business would benefit from marketing their more expensive items to this customer group.\nCluster 3 - Customers in cluster three have higher total sales and number of orders, but their average per-order value is not very high. They most likely purchase frequently and would benefit from low to mid priced item recommendations to increase or maintain these customers’ engagement.\nCluster 4 - These customers have the lowest total sales, a lower number of orders, and low per-order sales amount. This customer group should not be a focus of the business since the return on investment would be fairly small compared to the efforts of creating a marketing campaign.\n\n\nIn conclusion, this paper has explored the application of k-means clustering in the realm of e-commerce, highlighting its potential to revolutionize how businesses personalize their services and marketing strategies. By effectively segmenting customers into discrete clusters based on shared characteristics, k-means clustering offers a valuable tool for understanding consumer behavior, optimizing marketing efforts, and enhancing customer engagement. It has been demonstrated that k-means clustering can be applied to various real-world scenarios, including customer profiling, anomaly detection, recommendation systems, image compression, and geographic data analysis. This research has not only shed light on the practical advantages that e-commerce enterprises can gain from using k-means clustering but has also emphasized the need for continued exploration and refinement of clustering techniques to meet the evolving demands of the e-commerce industry. As data continues to grow at an unprecedented rate, k-means clustering remains a valuable tool for data-driven decision-making, enabling businesses to stay competitive and deliver personalized experiences to their customers. However, it’s important to acknowledge the challenges and complexities associated with implementing k-means clustering, such as handling large and noisy datasets, ensuring robustness against outliers, and determining the appropriate number of clusters (\\(K\\)). The paper has addressed these issues and provided solutions to enhance the reliability and effectiveness of k-means clustering in e-commerce analytics. By aligning our findings and future scope with the specific analysis and methods detailed in this paper, we anticipate a continued and exciting journey of discovery and innovation in the e-commerce landscape."
  },
  {
    "objectID": "FutureWorks.html",
    "href": "FutureWorks.html",
    "title": "Future Works",
    "section": "",
    "text": "The k-means clustering method has been an idea, discussed and explored by mathematicians, for over half a century. One of the first instances comes from a study in 1965 which outlines a more modern approach to the method, including an approach to find the ideal number of clusters; presented by Geoffrey Ball and David Hall (Ball and Hall 1965). Over the past sixty years, improvements and additions have been made to the base k-means methodology in order to optimize performance and increase scalability. Today, there are many different approaches to the method, including but not limited to finding the optimal value of K, and mixture models to increase accuracy. With the increase in general computing power and the development of additional algorithms, practitioners have implemented different distance functions for assigning data to centroids, included additional methods to refine data during preprocessing, supporting processes to support the base algorithm, and generally increased the accuracy and performance of a very basic approach to unsupervised data mining. Given the time constraint of this study, most of the improvements and suggested approaches to the k-means algorithm were not able to be explored. Future studies may include a comparison between different distance formulas such as the Mahatten distance and Chebyshev distance, the use of additional k selection methods such as the Empirical and Silhouette methods, and the use of principal component analysis or other dimensionality reduction techniques to improve efficiency and overall clustering results.\n\n\n\n\nBall, Geoffrey H., and David J. Hall. 1965. “ISODATA, a NOVEL METHOD OF DATA ANALYSIS AND PATTERN CLASSIFICATION.” In. https://api.semanticscholar.org/CorpusID:53887616."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ali, Huda H., and Lubna Emad Kadhum. 2017. “K- Means Clustering\nAlgorithm Applications in Data Mining and Pattern Recognition.”\nIn. https://api.semanticscholar.org/CorpusID:36213323.\n\n\nBall, Geoffrey H., and David J. Hall. 1965. “ISODATA, a NOVEL\nMETHOD OF DATA ANALYSIS AND PATTERN CLASSIFICATION.” In. https://api.semanticscholar.org/CorpusID:53887616.\n\n\nMorissette, Sylvain, Laurence AND Chartier. 2013. “The k-Means\nClustering Technique: General Considerations and Implementation in\nMathematica.” Tutorials in Quantitative Methods for\nPsychology 9 (1): 15–24. https://doi.org/10.20982/tqmp.09.1.p015.\n\n\n“Online Retail.” 2015. UCI Machine Learning\nRepository.\n\n\nSteinley, Douglas. 2006. “K-Means Clustering: A Half-Century\nSynthesis.” British Journal of Mathematical & Statistical\nPsychology 59 (May): 1–34. https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2.\n\n\nŽalik, Krista Rizman. 2008. “An Efficient k′-Means Clustering\nAlgorithm.” Pattern Recognition Letters 29 (9): 1385–91.\nhttps://doi.org/https://doi.org/10.1016/j.patrec.2008.02.014."
  }
]