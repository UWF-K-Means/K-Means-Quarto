---
title: "K-MeansPresentation"
format: revealjs
  incremental: true 
---

##An Introduction to K-Means Clustering

::: {.nonincremental}

Finding meaningful patterns within data has become unobtrusive as data collection and management continues to grow.

. . .

K-Means Clustering caters to such voluminous & unlabeled data, utilizing

::: {.fragment .fade-up}
-   Unsupervised learning
-   Clustering
-   Variance minimization techniques: distance calculations
:::

---
::: {.nonincremental}

There are 5 main steps to execute the K-Means Clustering method.

::: {.fragment .fade-up}
1. Assign all data points to a cluster noted as 'K'.
2. Set 'K' random seed points.
3. Reassign all data points to a cluster using distance to seed point.
4. Compute the mean center of each new cluster noted as a centroid.
5. Loop the re-assignment step of all data points to a new centroid based on its minimized relocation, until the centroid does not change.
:::
. . .

We will employ the K-means clustering algorithm to gain insights on customer segmentation in eCommerce data for a retail store based in the UK.


:::{.notes}

We will discuss the 5 main steps of the method and apply it to an eCommerce data set but first it is necessary to introduce some components needed to set up the K-Means Clustering analysis.

K-Means Clustering will be used to classify consumer bases into discrete clusters based on shared characteristics. These clusters will be interpreted and discussed at the end of the analysis.
:::

#Components of K-Means Clustering
##Unsupervised learning 
::: {.incremental}

-One of the longer-standing methods of autonomous data categorization comes in the form of the K-Means Clustering algorithm, and is generally selected for its simplicity and speed (Žalik 2008).

-The K-means method is considered unsupervised machine learning because the outcome is rather data driven , which is preferred as the formulation can be adjusted based on any changing dimensions in data.

-Used when data is to be categorized or grouped without the assistance of historical results or human intervention.

:::{.notes}
The autonomous reassignment of data to a centroid is the act that makes this method a form of unsupervised learning. 

:::

## Clustering
::: {.incremental}
-The goal of clustering is to create partitions in the data that are meaningful.

-These data are clustered based on randomly set seed centers, designated as k=4.

:::{.absolute bottom=100 right=100 .fragment}

![centroids](centroids.png){feg-alt=" k=4, the seed point are green, forming 4 respective clusters"}

:::

## Euclidean Distance 
::: {.nonincremental}

:::: {.columns}

:::{.column width ="50%" .fragment} Left column

-   Euclidean distance can be employed as a key metric to measure the dissimilarity between data points, facilitating the grouping of similar data instances into clusters using the k-means clustering algorithm.

:::

::: {.column width ="50%" .fragment} Right column

$$
d(x,C_i)=sqrt(\sum_{i=1}^{N} (x_j−C_{ij})^2)
$$

Objective Function: The k-means clustering objective is to minimize the within-cluster variance. 

It is formulated as:

$$
d(x,C_i)=(\sum_{i=1}^{k}*\sum_{x \in C_i}^{}(||x-\mu_i||)^2)
$$
:::
::::
---

:::{.nonincremental}

-   $k$ is the number of clusters.

-   $C_i$ represents the number of points in the cluster $i$

-   $\mu_i$ represents the centroid mean of cluster $i$

-   In this context, similarity is inversely related to the Euclidean distance

-   The smaller the distance, the greater the similarity between objects.

---
::: {.nonincremental}
- K-Means Clustering reassigns the data points to each cluster based on the Euclidean Distance calculation

![centroidocs/clusters.png.png){feg-alt=" ..."}


##Executing Method Steps
::: {.nonincremental}
-Determine K, in step 1

::: notes
To initiate the algorithm, you must specify K in step 1 the initial number of clusters and their initial centers. We will discuss the Elbow method in this case.
:::

##Finding K
::: {.incremental}
The Elbow Method is used to find where the within cluster sum of squares (total WCSS) appears to decrease less and less. This gives us the ideal starting value of $4$ for the initial number of clusters in the k-means model.

```{r code-line-numbers="129-134"}
# Create an empty vector to store WCSS values
wcss <- vector("numeric", length = 10)

# Iterate over a range of K values (e.g., from 1 to 10)
for (i in 1:10) {
  model <- kmeans(df_norm[c("Sales", "Orders", "AvgSale")], centers = i, nstart = 10)
  wcss[i] <- ceiling(model$tot.withinss)
}

# Plot the WCSS values against the number of clusters
ggplot(data.frame(K=1:10, WCSS=wcss), aes(x=K, y=WCSS)) +
  geom_line() +
  geom_point() +
  labs(title="Elbow Method to Find Optimal K", x="Number of Clusters (K)", y="Within-Cluster-Sum-of-Squares (WCSS)") +
  scale_x_continuous(breaks = seq(0, 10, by = 1))
```

(Do we want to show how data was worked and set up and why (remove uk, created summary items, normalized data?)

#Analysis \## Use eCommerce data to show execution of next steps \## Cluster Interpretation ##Conclusion

-There are many methods to choose from to evaluate metrics for clustering. K-Means Clustering is often used in tandem with other clustering techniques.
