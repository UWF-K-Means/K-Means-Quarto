[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "K-Means Clustering and Its Place In Ecommerce",
    "section": "",
    "text": "Slides\n\nIntroduction\nPlatforms for e-commerce gather gigabytes, terabytes, and even petabytes of information about consumer interactions, product preferences, and transactional histories. There is a heightened demand for more robust analytical processes as data continues growing at an unprecedented rate. The use of unsupervised machine learning and efficient clustering algorithms is crucial to the marketing and e-commerce industry when it comes to delivering meaningful and accurate communications to consumers. If properly tapped into, this abundance of data has the potential to fundamentally alter how internet businesses personalize and enhance their services and growth strategies. In this setting, k-means clustering, a cornerstone of data science, emerges as a guiding light, promising to uncover the hidden structures in e-commerce information and offer a deeper comprehension of consumer behavior. The volume of data will continue to grow as more people have access to an ever expanding internet for social media, online shopping, and even remote working opportunities, and so will the need for efficient and effective consumer marketing methods. Clustering is an efficient method to uncover tactical patterns that may be buried in a sea of unlabeled data sets [1]. The k-means clustering method takes care of the disadvantages seen in most other clustering classifications and has the ability to handle large amounts of data efficiently while considering many different attributes. Clustering techniques are mostly useful in exploratory data analysis and data mining however the k-means clustering is particularly known for its simplicity, and low computational complexity and seen as the most popular algorithm suitable in various applications.\nUnsupervised machine learning methods are powerful statistical modeling techniques that allow for data to be categorized or grouped without the assistance of historical results or human intervention. One of the longer-standing methods of autonomous data categorization comes in the form of the k-means clustering algorithm, and is generally selected for its simplicity and speed [2]. The k-means method is considered unsupervised machine learning because the outcome is rather data driven, which is preferred as the formulation can be adjusted based on any changing dimensions in data.\nThe basic method of the k-means algorithm is to randomly identify a centroid for every cluster as k, and use it as a reference point to assign all data points that retain the greatest similarity to that center, creating a cluster. The Euclidean distance considers the maximum distance between cluster means and each data object; it is used to identify the cluster each object best belongs to. This is a unique application because the centroid is updated every iteration by calculating the new mean with those new cluster members, nearly perfecting the requirement for within cluster similarities. The idea considers all of the data as a mixed batch of distributions, from Gaussian to Poisson and so on, where minimizing the variance within a cluster to identify an even more defined sub-population, is partioning all the data of each type of distribution in the batch into obvious clusters [3]. Our examination provides a thorough comprehension of its mathematical foundations and clustering concepts. This study will show how k-means may be used to classify consumer bases into discrete clusters based on shared characteristics in e-commerce data sets.\nK-means clustering helps organizations better target their marketing efforts and increase return on investment by defining client segments in the context of marketing. For space-efficient picture compression, image processing uses k-means, which is important for web graphics and mobile applications. K-means excels in anomaly detection in fraud detection, protecting financial systems and networks. Additionally, it is used by recommendation engines to customize the distribution of content, increasing user happiness and engagement. K-means promotes patient profiling in the healthcare industry, advancing individualized treatments and medication development. Natural language processing uses its strength to group text data, improving content categorization and information retrieval. K-means is a tool for geographic data analysis that provides regional understanding for urban planning and environmental studies.\nThe objective of the study is to explore how k-means clustering is used in a real-world e-commerce scenario. It will also highlight the practical advantages that e-commerce businesses can gain from applying k-means clustering techniques using real data. This study examines k-means clustering’s promise as well as the complexities and limitations of doing so on e-commerce data. Managing large and noisy data sets, guaranteeing robustness against outliers, and determining an appropriate value for the “K” parameter are a few of the difficulties that are addressed in during the process.\n\n\n\n\n[1] H. H. Ali and L. E. Kadhum, “K- means clustering algorithm applications in data mining and pattern recognition,” 2017. Available: https://api.semanticscholar.org/CorpusID:36213323\n\n\n[2] K. R. Žalik, “An efficient k′-means clustering algorithm,” Pattern Recognition Letters, vol. 29, no. 9, pp. 1385–1391, 2008, doi: https://doi.org/10.1016/j.patrec.2008.02.014.\n\n\n[3] S. Morissette Laurence AND Chartier, “The k-means clustering technique: General considerations and implementation in mathematica,” Tutorials in Quantitative Methods for Psychology, vol. 9, no. 1, pp. 15–24, 2013, doi: 10.20982/tqmp.09.1.p015."
  },
  {
    "objectID": "Methods.html",
    "href": "Methods.html",
    "title": "Overview",
    "section": "",
    "text": "The k-means method, as described by Steinley, is designed to partition data that is \\(N\\) objects having measurements on \\(P\\) variables, into \\(K\\) classes (\\(C_1\\),\\(C_2\\),\\(C_3\\),…,\\(C_k\\)), where \\(C_k\\) is the set of \\(n_k\\) objects in cluster \\(k\\), where \\(K\\) is given [1]. In this context, the uppercase \\(K\\) denotes the number of clusters where the lowercase \\(k\\) refers to a specific cluster. The method computes seed points, which are randomly assigned reference points called centroids. Taking note that a centroid is not considered a physically observed data point, but rather a location in the center of a three dimensional space that is used as a reference point for a cluster. The distance between the centroids and the surrounding data points is then calculated using a distance formula. Data points are measured and assigned to the closest centroid. Then the mean is derived for each newly created cluster which becomes the new centroid of each cluster. The process is then repeated until there are no changes in the distance and mean of data points to the assigned cluster centroids. While there are different distance calculations to choose from, like the Manhattan Distance or Chebyshev Distance, the most commonly used is the Euclidean Distance.\nThe k-means algorithm relies on Euclidean distance as a measure of similarity between data objects. Euclidean distance is a powerful tool for measuring similarity in various applications, including image processing, pattern recognition, and clustering, such as in the k-means algorithm, where it helps partition data into well-defined clusters based on their spatial relationships in the feature space [2]. Euclidean distance can be employed as a key metric to measure the dissimilarity between data points, facilitating the grouping of similar data instances into clusters using the k-means clustering algorithm. This allows us to uncover patterns and relationships within the defined set of data, which is critical for achieving the research objectives.\nIn this context, similarity is inversely related to the Euclidean distance; in other words, the smaller the distance, the greater the similarity between objects [3]. To initiate the algorithm, an initial number of clusters \\((K)\\) and their initial centers are selected. The algorithm then iteratively adjusts these cluster centers based on the similarity between data objects and cluster centers. Clustering continues until the objective function converges, signaling the end of the process and yielding the final result.\n\n\n\n\n[1] D. Steinley, “K-means clustering: A half-century synthesis,” British Journal of Mathematical & Statistical Psychology, vol. 59, pp. 1–34, May 2006, Available: https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2\n\n\n[2] J. Yang, Y.-K. Wang, X. Yao, and C.-T. Lin, “Adaptive initialization method for k-means algorithm,” Frontiers in Artificial Intelligence, vol. 4, 2021, doi: 10.3389/frai.2021.740817.\n\n\n[3] C. M. Bishop, “Pattern recognition and machine learning,” SpringerLink. Springer New York, 2006. Available: https://link.springer.com/book/9780387310732"
  },
  {
    "objectID": "Methods2.html",
    "href": "Methods2.html",
    "title": "Euclidean Distance & Finding K",
    "section": "",
    "text": "The formula for calculating the Euclidean distance [1] between a data object and a cluster center can be expressed as follows: \\[\nd(x,C_i)=sqrt(\\sum_{i=1}^{N} (x_j−C_{ij})^2)\n\\]\nObjective function: The k-means clustering objective is to minimize the within-cluster variance. It is formulated as: \\[\nd(x,C_i)=(\\sum_{i=1}^{k}*\\sum_{x \\in C_i}^{}(||x-\\mu_i||)^2)\n\\]\nWhere: \\(k\\) is the number of clusters.\n\\(C_i\\) represents the number of points in the cluster \\(i\\)\n\\(\\mu_i\\) represents the centroid mean of cluster \\(i\\)\nThe objective function is the sum of squared Euclidean distances from each point in a cluster to its centroid. The goal is to find cluster assignments and centroids that minimize this objective.\nTo determine \\(k\\), the number of clusters to initially set, there are a few approaches such as the empirical, elbow, and Silhouette methods. The empirical method assigns the number of clusters \\(\\approx \\frac{n^{\\frac{1}{2}}}{2}\\) for a data set of \\(n\\) observations [1]. The elbow method is based on the idea that increasing the number of clusters, \\(k\\), will reduce the sum of within-cluster variance. After plotting the sum of within-cluster variances as a function of the number of clusters, the inflection point is determined using the turning point in the curve.\n\n\n\n\n[1] C. M. Bishop, “Pattern recognition and machine learning,” SpringerLink. Springer New York, 2006. Available: https://link.springer.com/book/9780387310732"
  },
  {
    "objectID": "Methods3.html",
    "href": "Methods3.html",
    "title": "Step by Step",
    "section": "",
    "text": "The k-means algorithm for partitioning can be executed in 5 main steps.\n\nPartition \\(N\\) objects, or observations, into \\(K\\) nonempty subsets [1]. Each cluster must not be empty and will have a different classification. This is \\(K\\), if \\(k=4\\) that means there will be four clusters.\nThen compute seed points, which are randomly assigned reference points called \\(centroids\\) [2]. In the following figures we will see \\(K=4\\), which are highlighted in green.\nAssign each object to the cluster with the nearest assigned \\(centroid\\). Figure 1 shows the Euclidean distance calculation [3] as a red line connecting each observation to one of the 4 \\(centroids\\).\n\n\n\n\nFigure 1: Centroids [4]\n\n\n\nAdjust the \\(centroid\\) location using the Euclidean distance formula to minimize the distance of each data observation to its assigned \\(centroid\\) . Figure 2 shows how the Euclidean distance drives some cluster members to reassign to a new cluster, influencing the \\(centroids\\) positions. This updates the cluster mean, \\(\\mu_i\\), in turn relocating the \\(centroids\\) from where they were seen in Figure 1.\nThe final step is to repeat steps 2-4 until there is no change in distance between the observations and the \\(centroids\\) [5].\n\n\n\n\nFigure 2: Clusters [4]\n\n\nThe result uncovers 4 distinct clusters containing the most similar observations within groups.\n\n\n\n\n[1] H. H. Ali and L. E. Kadhum, “K- means clustering algorithm applications in data mining and pattern recognition,” 2017. Available: https://api.semanticscholar.org/CorpusID:36213323\n\n\n[2] A. M. Mehar, K. Matawie, and A. Maeder, “Determining an optimal value of k in k-means clustering,” in 2013 IEEE international conference on bioinformatics and biomedicine, 2013, pp. 51–55. doi: 10.1109/BIBM.2013.6732734.\n\n\n[3] C. M. Bishop, “Pattern recognition and machine learning,” SpringerLink. Springer New York, 2006. Available: https://link.springer.com/book/9780387310732\n\n\n[4] Ph. D. Andrey A. Shambalin, “Visuals and animations, k-means clustering.” Available: https://shabal.in/visuals/kmeans/1.html\n\n\n[5] S. Morissette Laurence AND Chartier, “The k-means clustering technique: General considerations and implementation in mathematica,” Tutorials in Quantitative Methods for Psychology, vol. 9, no. 1, pp. 15–24, 2013, doi: 10.20982/tqmp.09.1.p015."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "To begin, an appropriate data source is identified. The data used in this analysis comes from UC Irvine’s Machine Learning Repository, [1], containing \\(541,909\\) transactional observations. In order to maintain a healthy set of usable data, disruptive null values from the base data are removed and a subsequent view of data descriptions are explored.\n\n\n\n\n\nCode\ndata_nulls_removed <- na.omit(data)\nsave(data_nulls_removed, file = \"data_nulls_removed.RData\")\ndata_summary <- data.frame(InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country)\ndata_summary\n\n\n\n\n  \n\n\n\n\n\n\nThe descriptions show that the data, excluding observations with null values, consists of \\(406,829\\) total observations. This specific data set originates from an online retailer located in the United Kingdom, but provides retail service to many different countries and regions. From the summary of the chosen data set, \\(8\\) variables are identified: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country.\n\n\nCode\nhead(data_nulls_removed,10)\n\n\n\n\n  \n\n\n\nThe frequency of purchases by Country may help provide some context about the regional distribution of customers, as well as the possible variety of customers:\n\n\nCode\ntopCountries_all <- data_nulls_removed %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE)\n\nggplot(data = topCountries_all, mapping = aes(x=Country, y=n, fill=Country)) + \n  geom_bar(stat=\"identity\") +\n  labs(title = \"Top 10 Countries\", y=\"Total Lifetime Transactions\") +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\nIt quickly becomes apparent that an overwhelming amount of data comes from the UK, as expected, which could skew the results during any analysis. In order to avoid this, a subset of data is generated which excludes data associated with the UK.\n\n\nCode\ndata_no_uk <- data_nulls_removed[data_nulls_removed$Country != \"United Kingdom\",]\nsave(data_no_uk, file = \"data_no_uk.RData\")\n\n\nWith the UK data removed, the top \\(10\\) frequencies are examined:\n\n\nCode\ntopCountries <- data_no_uk %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE) \n\nggplot(data = topCountries, mapping = aes(x=Country, y=n, fill=Country)) + \n  geom_bar(stat=\"identity\") +\n  labs(title = \"Top 10 Countries (excluding UK)\", y=\"Total Lifetime Transactions\") +\n  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))\n\n\n\n\n\nNow that the distribution of data does not primarily fall under a single country’s customer base, the analysis can proceed.\nReferring back to the view of the data table, there appears to be more than \\(1\\) observation per InvoiceNo. This means that the data is only grouped by StockCode (product) and InvoiceNo. In order to create a better view of each customer and their overall spending habits, some simple calculated fields are created and assigned back to the data source.\nHere the Quantity and UnitPrice columns are multiplied together and assigned to a new variable called “total” in order to obtain the total cost of the items purchased per observation:\n\n\nCode\ndata_no_uk <- data_no_uk %>% mutate(total = data_no_uk$Quantity*data_no_uk$UnitPrice)\nsave(data_no_uk, file = \"data_no_uk.RData\")\n\n\n\n\n\n\n  \n\n\n\nThe data is beginning to look usable and gives a more wholistic view of the customers, but there are still some improvements that can be made to ensure bias is avoided. Since the interest lies in purchasing-customer clustering, any customers or invoices that include returns or negative values for Quantity can be excluded from the data.\n\n\nCode\ndata_no_uk <- data_no_uk %>% subset(Quantity > 0)\nsave(data_no_uk, file = \"data_no_uk.RData\")\n\n\nFor the remainder of the analysis, only the applicable columns of interest are retained and assigned to a new data frame called df1.\n\n\nCode\ndf1 <- data_no_uk %>% dplyr::select(Quantity,UnitPrice,CustomerID,InvoiceDate,total)\nsave(df1, file = \"df1.RData\")\n\n\n\n\n\n\n  \n\n\n\nNext, a sum total of sales from the previously derived column total is created as Sales, along with a unique count of InvoiceDate to provide the number of orders made, labeled Orders; then the difference of those two variables is found and labeled AvgSale. The data is then grouped by CustomerID to decrease the granularity and provide a more complete view of each customer and their purchasing history and habits. This new set of data will be labeled customer_data:\n\n\nCode\ncustomer_data <- df1 %>% group_by(CustomerID) %>% summarize(Sales=sum(total), Orders=length(unique(InvoiceDate))) %>% mutate(AvgSale=Sales/Orders)\nsave(customer_data, file = \"customer_data.RData\")\n\n\n\n\n\n\n  \n\n\n\n\n\n\nBased on the newly compressed data, customers’ behaviors can be visualized and subsequently used in a meaningful clustering exercise:\n\n\n\n\n\n\nFigure 1: Top 5 Avg Sales\n\n\n\n\n\n\n\nFigure 2: Bottom 5 Avg Sales\n\n\n\n\n\n\n\n\n\nFigure 3: Top 5 Sales\n\n\n\n\n\n\n\nFigure 4: Bottom 5 Sales\n\n\n\n\n\n\n\n\n\nFigure 5: Top 5 Order Frequencies\n\n\n\n\n\nHaving a better view and understanding of the data format and structure, the next step is to normalize the data in order to produce a normal distribution across the different variables. This is done by ranking the Sales, Orders, and AvgSale. Once these values have been ranked, they can be scaled for the final output. This final output is called df_norm.\n\n\nCode\ndf_ranked <- customer_data %>% mutate(Sales=rank(Sales), Orders=rank(Orders, ties.method = \"first\"), AvgSale=rank(AvgSale))\nsave(df_ranked, file = \"df_ranked.RData\")\n\ndf_norm <- df_ranked %>% mutate(Sales=scale(Sales), Orders=scale(Orders), AvgSale=scale(AvgSale))\nsave(df_norm, file = \"df_norm.RData\")\n\n\nThe resulting data is verified as normal by checking the standard deviation of each variable. Noting that CustomerID will not be normalized since it is an identifier for each record:\n\n\n\n\n  \n\n\n\nNow that the data is normalized, a value is determined for \\(k\\) using the elbow method:\n\n\nCode\n# Create an empty vector to store WCSS values\nwcss <- vector(\"numeric\", length = 10)\n\n# Iterate over a range of K values (e.g., from 1 to 10)\nfor (i in 1:10) {\n  model <- kmeans(df_norm[c(\"Sales\", \"Orders\", \"AvgSale\")], centers = i, nstart = 10)\n  wcss[i] <- ceiling(model$tot.withinss)\n}\n\n# Plot the WCSS values against the number of clusters\nggplot(data.frame(K=1:10, WCSS=wcss), aes(x=K, y=WCSS)) +\n  geom_line() +\n  geom_point() +\n  labs(title=\"Elbow Method to Find Optimal K\", x=\"Number of Clusters (K)\", y=\"Within-Cluster-Sum-of-Squares (WCSS)\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1))\n\n\n\n\n\nBased on the results of the Elbow Method, above, the total within cluster sum of squares appears to decrease the least starting at \\(4\\). This gives us the ideal starting value for the initial number of clusters in the k-means model.\nTo build the model, a seed is set (in order to make the results reproducible) and the model function is initialized:\n\n\nCode\nset.seed(100)\nmodel1 <- kmeans(df_norm[c(\"Sales\", \"Orders\", \"AvgSale\")],4)\nsave(model1, file = \"model1.RData\")\n\n\nThe model results are presented here:\n\n\n\n\n  \n\n\n\nWhile the model output is useful, the best visual representation of the clusters is through scatter plot.\nPlot and view cluster results:\n\n\nCode\nfviz_cluster(model1, data = df_norm,\n             geom = \"point\",\n             ellipse.type = \"convex\",\n             ggtheme = theme_bw()\n)\n\n\n\n\n\nSince the model utilizes more than \\(2\\) variables and the plot plane is two-dimensional, the axis of the graph, above, are based on principle components instead of the individual model variables.\nThe graph below shows the customers in relation to all three model variables in a three-dimensional scatter plot:\n\n\n\n\n\nCode\nplot_ly(customer_data, x=~Orders, y=~Sales, z=~AvgSale,\n        color=~Cluster2,\n        colors=c(\"red\",\"green\",\"blue\",\"violet\")) %>% \n  add_markers(size=2) %>% \n  layout(scene = list(xaxis=axx,yaxis=axy,zaxis=axz))\n\n\n\n\nInteractive graph (left-click to rotate / right-click to move)\n\n\n\n\n\n\n[1] “Online Retail.” UCI Machine Learning Repository, 2015."
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Cluster Explanation\n\nCluster 1 - Customers in this cluster have lower total sales, a low number of orders, and relatively average per-order sales. These customers could potentially be newer customers and would benefit from some targeted marketing in order to drive engagement and increase total sales.\nCluster 2 - The customers in this cluster have average total sales with slightly below average number of orders and very high per-order sales. These customers buy expensive items but not very frequently. The business would benefit from marketing their more expensive items to this customer group.\nCluster 3 - Customers in cluster three have higher total sales and number of orders, but their average per-order value is not very high. They most likely purchase frequently and would benefit from low to mid priced item recommendations to increase or maintain these customers’ engagement.\nCluster 4 - These customers have the lowest total sales, a lower number of orders, and low per-order sales amount. This customer group should not be a focus of the business since the return on investment would be fairly small compared to the efforts of creating a marketing campaign.\n\n\nIn conclusion, this paper has explored the application of k-means clustering in the realm of e-commerce, highlighting its potential to revolutionize how businesses personalize their services and marketing strategies. By effectively segmenting customers into discrete clusters based on shared characteristics, k-means clustering offers a valuable tool for understanding consumer behavior, optimizing marketing efforts, and enhancing customer engagement. It has been demonstrated that k-means clustering can be useful in a real-world scenario. This research has not only shed light on the practical advantages that e-commerce enterprises can gain from using k-means clustering but has also emphasized the need for continued exploration and refinement of clustering techniques to meet the evolving demands of the e-commerce industry. As data continues to grow at an unprecedented rate, k-means clustering remains a valuable tool for data-driven decision-making, enabling businesses to stay competitive and deliver personalized experiences to their customers. However, it’s important to acknowledge the challenges and complexities associated with implementing k-means clustering, such as handling large and noisy datasets, ensuring robustness against outliers, and determining the appropriate number of clusters (\\(k\\)). The approach in this exploration has addressed some of these issues to enhance the reliability and effectiveness of k-means clustering in e-commerce analytics. By aligning our findings and future scope with the specific analysis and methods detailed in this paper, we anticipate a continued and exciting journey of discovery and innovation in the e-commerce landscape."
  },
  {
    "objectID": "FutureWorks.html",
    "href": "FutureWorks.html",
    "title": "Future Works",
    "section": "",
    "text": "The k-means clustering method has been an idea, discussed and explored by mathematicians, for over half a century. One of the first instances comes from a study in 1965 which outlines a more modern approach to the method, including an approach to find the ideal number of clusters; presented by Geoffrey Ball and David Hall [1]. Over the past sixty years, improvements and additions have been made to the base k-means methodology in order to optimize performance and increase scalability. Today, there are many different approaches to the method, including but not limited to finding the optimal value of K, and mixture models to increase accuracy. With the increase in general computing power and the development of additional algorithms, practitioners have implemented different distance functions for assigning data to centroids, included additional methods to refine data during preprocessing, supporting processes to support the base algorithm, and generally increased the accuracy and performance of a very basic approach to unsupervised data mining. Given the time constraint of this study, most of the improvements and suggested approaches to the k-means algorithm were not able to be explored. Future studies may include a comparison between different distance formulas such as the Mahatten distance and Chebyshev distance, the use of additional k selection methods such as the Empirical and Silhouette methods, and the use of principal component analysis or other dimensionality reduction techniques to improve efficiency and overall clustering results.\n\n\n\n\n[1] G. H. Ball and D. J. Hall, “ISODATA, a NOVEL METHOD OF DATA ANALYSIS AND PATTERN CLASSIFICATION,” 1965. Available: https://api.semanticscholar.org/CorpusID:53887616"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] K.\nR. Žalik, “An efficient k′-means clustering algorithm,”\nPattern Recognition Letters, vol. 29, no. 9, pp. 1385–1391,\n2008, doi: https://doi.org/10.1016/j.patrec.2008.02.014.\n\n\n[2] D.\nSteinley, “K-means clustering: A half-century synthesis,”\nBritish Journal of Mathematical & Statistical Psychology,\nvol. 59, pp. 1–34, May 2006, Available: https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2\n\n\n[3] A.\nM. Mehar, K. Matawie, and A. Maeder, “Determining an optimal value\nof k in k-means clustering,” in 2013 IEEE international\nconference on bioinformatics and biomedicine, 2013, pp. 51–55. doi:\n10.1109/BIBM.2013.6732734.\n\n\n[4] H.\nH. Ali and L. E. Kadhum, “K- means clustering algorithm\napplications in data mining and pattern recognition,” 2017.\nAvailable: https://api.semanticscholar.org/CorpusID:36213323\n\n\n[5] J.\nYang, Y.-K. Wang, X. Yao, and C.-T. Lin, “Adaptive initialization\nmethod for k-means algorithm,” Frontiers in Artificial\nIntelligence, vol. 4, 2021, doi: 10.3389/frai.2021.740817.\n\n\n[6] S.\nMorissette Laurence AND Chartier, “The k-means clustering\ntechnique: General considerations and implementation in\nmathematica,” Tutorials in Quantitative Methods for\nPsychology, vol. 9, no. 1, pp. 15–24, 2013, doi: 10.20982/tqmp.09.1.p015.\n\n\n[7] “Online Retail.” UCI\nMachine Learning Repository, 2015.\n\n\n[8] G.\nH. Ball and D. J. Hall, “ISODATA, a NOVEL METHOD OF DATA ANALYSIS\nAND PATTERN CLASSIFICATION,” 1965. Available: https://api.semanticscholar.org/CorpusID:53887616\n\n\n[9] Ph.\nD. Andrey A. Shambalin, “Visuals and animations, k-means\nclustering.” Available: https://shabal.in/visuals/kmeans/1.html\n\n\n[10] C.\nM. Bishop, “Pattern recognition and machine learning,”\nSpringerLink. Springer New York, 2006. Available: https://link.springer.com/book/9780387310732\n\n\n[11] L.\nHenry and H. Wickham, Rlang: Functions for base types and core r and\n’tidyverse’ features. 2023. Available: https://rlang.r-lib.org\n\n\n[12] H.\nWickham, R. François, L. Henry, K. Müller, and D. Vaughan, Dplyr: A\ngrammar of data manipulation. 2023. Available: https://dplyr.tidyverse.org\n\n\n[13] H.\nWickham et al., “Welcome to the tidyverse,” Journal of Open Source\nSoftware, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686.\n\n\n[14] H.\nWickham, ggplot2: Elegant graphics for data analysis.\nSpringer-Verlag New York, 2016. Available: https://ggplot2.tidyverse.org\n\n\n[15] D.\nLüdecke, sjPlot: Data visualization for statistics in social\nscience. 2023. Available: https://CRAN.R-project.org/package=sjPlot\n\n\n[16] H.\nWickham, D. Vaughan, and M. Girlich, Tidyr: Tidy messy data.\n2023. Available: https://tidyr.tidyverse.org\n\n\n[17] D.\nLüdecke, “Sjmisc: Data and variable transformation\nfunctions.” Journal of Open Source Software, vol. 3, no.\n26, p. 754, 2018, doi: 10.21105/joss.00754.\n\n\n[18] D.\nLüdecke, Sjlabelled: Labelled data utility functions (version\n1.2.0). 2022. doi: 10.5281/zenodo.1249215.\n\n\n[19] N.\nTierney and D. Cook, “Expanding tidy data principles to facilitate\nmissing data exploration, visualization and assessment of\nimputations,” Journal of Statistical Software, vol. 105,\nno. 7, pp. 1–31, 2023, doi: 10.18637/jss.v105.i07.\n\n\n[20] M.\nMaechler, P. Rousseeuw, A. Struyf, M. Hubert, and K. Hornik,\nCluster: Cluster analysis basics and extensions. 2023.\nAvailable: https://CRAN.R-project.org/package=cluster\n\n\n[21] A.\nKassambara and F. Mundt, Factoextra : Extract and visualize the\nresults of multivariate data analyses. 2020. Available: https://CRAN.R-project.org/web/packages/factoextra/index.html\n\n\n[22] C.\nSievert, Interactive web-based data visualization with r, plotly,\nand shiny. Chapman; Hall/CRC, 2020. Available: https://plotly-r.com\n\n\n[23] D.\nRobinson, A. Hayes, and S. Couch, Broom: Convert statistical objects\ninto tidy tibbles. 2023."
  },
  {
    "objectID": "all_code.html#the-following-packages-were-loaded-andor-used-during-the-analysis",
    "href": "all_code.html#the-following-packages-were-loaded-andor-used-during-the-analysis",
    "title": "R Code & Packages",
    "section": "The following packages were loaded and/or used during the analysis:",
    "text": "The following packages were loaded and/or used during the analysis:\n[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13]\n\n\n\n\n[1] L. Henry and H. Wickham, Rlang: Functions for base types and core r and ’tidyverse’ features. 2023. Available: https://rlang.r-lib.org\n\n\n[2] H. Wickham, R. François, L. Henry, K. Müller, and D. Vaughan, Dplyr: A grammar of data manipulation. 2023. Available: https://dplyr.tidyverse.org\n\n\n[3] H. Wickham et al., “Welcome to the tidyverse,” Journal of Open Source Software, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686.\n\n\n[4] H. Wickham, ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. Available: https://ggplot2.tidyverse.org\n\n\n[5] D. Lüdecke, sjPlot: Data visualization for statistics in social science. 2023. Available: https://CRAN.R-project.org/package=sjPlot\n\n\n[6] H. Wickham, D. Vaughan, and M. Girlich, Tidyr: Tidy messy data. 2023. Available: https://tidyr.tidyverse.org\n\n\n[7] D. Lüdecke, “Sjmisc: Data and variable transformation functions.” Journal of Open Source Software, vol. 3, no. 26, p. 754, 2018, doi: 10.21105/joss.00754.\n\n\n[8] D. Lüdecke, Sjlabelled: Labelled data utility functions (version 1.2.0). 2022. doi: 10.5281/zenodo.1249215.\n\n\n[9] N. Tierney and D. Cook, “Expanding tidy data principles to facilitate missing data exploration, visualization and assessment of imputations,” Journal of Statistical Software, vol. 105, no. 7, pp. 1–31, 2023, doi: 10.18637/jss.v105.i07.\n\n\n[10] M. Maechler, P. Rousseeuw, A. Struyf, M. Hubert, and K. Hornik, Cluster: Cluster analysis basics and extensions. 2023. Available: https://CRAN.R-project.org/package=cluster\n\n\n[11] A. Kassambara and F. Mundt, Factoextra : Extract and visualize the results of multivariate data analyses. 2020. Available: https://CRAN.R-project.org/web/packages/factoextra/index.html\n\n\n[12] C. Sievert, Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC, 2020. Available: https://plotly-r.com\n\n\n[13] D. Robinson, A. Hayes, and S. Couch, Broom: Convert statistical objects into tidy tibbles. 2023."
  }
]