---
title: "K-Means Presentation"
format: 
  revealjs:
    incremental: true
    df-print: kable
embed-resources: true
---

```{r}
load("df_norm.RData")
load("data_nulls_removed.RData")
load("data_no_uk.RData")
load("customer_data.RData")
load("customer_data2.RData")
library(broom)
library(tidyverse)
library(knitr)
library(factoextra)
library(plotly)
```

## A Deep Dive Into K-Means Clustering

::: incremental
Finding meaningful patterns within data has become obtrusive as data collection and management continues to grow at an unprecedented rate

-   K-means clustering caters to highly unpredictable data
-   Finds inexplicitly labeled groups
-   May provide unknown/hidden business insights
:::

::: notes
There is a heightened demand for more robust analytical processes as data continues growing at a fast pace

Unlabeled data is data that may have inexplicitly labeled groups or patterns suggesting diverse sub-populations within the data

We will explore the components of k-means clustering and learn how finding these "clusters" or sub-populations can be utilized within an eCommerce business to improve customer marketing and other strategies
:::

## Key Concepts

By the end of this presentation we will have discussed the following concepts of k-means clustering:

::: {.fragment .fade-up}
-   Unsupervised learning
-   Clustering
-   Calculating distance, specifically using Euclidean Distance
-   Example interpretation of results
:::

::: notes
Unsupervised learning, Clustering, and the Euclidean Distance are key components we will discuss in this presentation in order to fully understand how we will employ the k-means clustering algorithm.
:::

## The Data

-   "Online Retail" from UC Irvine's Machine Learning Repository
-   Online retailer based in UK
-   541,909 observations of customer transactions
-   8 variables: $InvoiceNo$, $StockCode$, $Description$, $Quantity$, $InvoiceDate$, $UnitPrice$, $CustomerID$, $Country$

::: notes
The data used in this analysis comes from UC Irvine's Machine Learning Repository "Online Retail" 2015.

We chose this data for its generous size and complexity, the vast amount of unlabeled data points are customer transactions.

We will first explore ways to better summarize individual customer transactions. With an end goal to classify consumer bases into discrete clusters based on shared characteristics.

These clusters will be interpreted and discussed at the end of the analysis.
:::

## Preparing the Data {.smaller}

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
head(data_nulls_removed,3)
```

::: notes
From a quick view of our data we can see 8 variables around a customer transaction: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, and Country.
:::

## Regional Distribution of Data {.smaller}

::: panel-tabset
### Top 10

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
topCountries_all <- data_nulls_removed %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE)

ggplot(data = topCountries_all, mapping = aes(x=Country, y=n, fill=Country)) + 
  geom_bar(stat="identity") +
  labs(title = "Top 10 Countries", y="Total Lifetime Transactions") +
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))
```

### Top 10 (UK removed)

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
topCountries <- data_no_uk %>% count(Country) %>% slice_max(n, n = 10, with_ties = FALSE) 

ggplot(data = topCountries, mapping = aes(x=Country, y=n, fill=Country)) + 
  geom_bar(stat="identity") +
  labs(title = "Top 10 Countries (excluding UK)", y="Total Lifetime Transactions") +
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))
```
:::

::: notes
The frequency of purchases by Country provide us with context about the regional distribution of customers.

Due to an overwhelming amount of data for the UK we will use a subset of the data excluding the UK's observations to avoid skewed results since this is a UK based company.

So far we see the distribution of customer transactions across several countries, and now we want to consider what factors in the data set can be used to better summarize these customer lifetime experience.
:::

## Cleaned Data {.smaller}

-   Created variables $Sales$, $Orders$, and $AvgSales$ to capture customer spending habits
-   Subset of data without UK sales
-   Removed null values and removed quantities $< 0$

::: {.fragment .fade-up}
```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
head(customer_data)
```
:::

::: notes
We created new variables to help depict a customers overall interactions with the retailer using descriptors: Quantity, Sales, and Average Sales.

We are interested in purchasing behavior so we will exclude returns to avoid negative values for Quantity along with other null values.
:::

## Visualizing Customer Interaction by Total Sales and Orders {.smaller}

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
top5AvgValues <- top_n(customer_data, 5, AvgSale)
top5Sales <- top_n(customer_data, 5, Sales)
top5OrderCount<- top_n(customer_data,5,Orders)
bottom5AvgValues <- top_n(customer_data, -5, AvgSale)
bottom5Sales <- top_n(customer_data, -5, Sales)
options(scipen = 999)
```

::: panel-tabset
## Total Sales

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
#| label: fig-charts2
#| layout-ncol: 2
#| fig-cap: 
#|   - "Top 5 Sales"
#|   - "Bottom 5 Sales"

ggplot(top5Sales, aes(x=reorder(CustomerID,-Sales),y=Sales, fill=as.factor(CustomerID))) + 
  geom_bar(stat="identity") +
  labs(title="Top 5 Customers by Total Sales", x="CustomerID",y="Lifetime Total Sales") +
  theme(plot.title = element_text(size = 20)) +
  guides(fill=guide_legend(title="CustomerID"))

ggplot(bottom5Sales, aes(x=reorder(CustomerID,-Sales),y=Sales, fill=as.factor(CustomerID))) +
  geom_bar(stat="identity") +
  labs(title="Bottom 5 Customers by Total Sales",x="CustomerID",y="Total Sales") +
  theme(plot.title = element_text(size = 20)) +
  guides(fill=guide_legend(title="CustomerID"))
```

## OrderCount

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}

ggplot(top5OrderCount, aes(x=reorder(CustomerID,-Orders),y=Orders, fill=as.factor(CustomerID))) + 
  geom_bar(stat="identity") +
  labs(title="Top 5 Customers by Order Count", x="CustomerID",y="Lifetime Total Orders") +
  theme(plot.title=element_text(size = 20)) +
  guides(fill=guide_legend(title="CustomerID"))
```
:::

::: notes
Quantity and price were used to create a 'total' column. 'Sales' is also created as the sum total derived from the new 'total' column Orders are calculated by Invoice date to account for all transactions.
:::

## Visualizing the Average Customer Interaction

```{r, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
#| label: fig-charts1
#| layout-ncol: 2
#| fig-cap: 
#|   - "Top 5 Avg Sales"
#|   - "Bottom 5 Avg Sales"

ggplot(top5AvgValues, aes(x=reorder(CustomerID,-AvgSale),y=AvgSale, fill=as.factor(CustomerID))) + 
  geom_bar(stat="identity") +
  labs(title="Top 5 Customers by Average Sale", x="CustomerID", y="Lifetime Average") +
  theme(plot.title=element_text(size = 20)) +
  guides(fill=guide_legend(title="CustomerID"))

ggplot(bottom5AvgValues, aes(x=reorder(CustomerID,-AvgSale),y=AvgSale, fill=as.factor(CustomerID))) + 
  geom_bar(stat="identity") +
  labs(title="Bottom 5 Customers by Average Sale", x="CustomerID",y="Lifetime Average") +
  theme(plot.title=element_text(size = 20)) +
  guides(fill=guide_legend(title="CustomerID"))
```

::: notes
We created an 'Average sale' column that is the difference of Sales and Orders to represent the lifetime average spent.
:::

## Introducing K-Means Clustering By Steps {.smaller}

::: nonincremental
There are 5 main steps to execute the k-means clustering method
:::

::: {.fragent .fade-up}
-   Set $k$, the ideal number of clusters
-   Assign each data point to a cluster by its proximity to the $centroid$
-   Compute each new $centorid$ by finding cluster average
-   Reassign all data points to a cluster calculating its distance to a new centroid
-   Establish final cluster members by looping the cluster assignment step of all data points to a new centroid until the centroid does not change
:::

::: notes
We will break down the following 5 basic steps of the k-means method followed by an example with the eCommerce data set

In summary, the steps are:

1.  Establish $k$ (number of clusters) and start by randomly marking a location in the data set to represent a central reference point

2.  Calculate the minimal distance from each data point to a centroid to designate premature cluster members

3.  Assign new centroids by computing the mean of each of the clusters

4.  Re-assign the data points to a cluster by their distance to the new centroid

5.  Lastly, as data points shift and centroid means are re-calculated, the process loops the assignment of data points to a cluster until the mean value for each centroid does not change

It is necessary to introduce some components needed to set up the k-means clustering analysis
:::

## Unsupervised Learning {.smaller}

::: incremental
-   Statistical modeling technique used to categorize/group data without the assistance of historical results or human intervention
-   Autonomous data categorization
-   Creates a data driven outcome
:::

::: notes
The autonomous reassignment of data to a centroid is the act that makes this method a form of unsupervised learning.

This is rather a data driven outcome as the data fall into different clusters, new centroids are calculated based on those members.

Initial centroids are generated randomly in this generic model for K-means clustering, we will discuss how other methods intend to improve the model with a modified centroid selection.
:::

## Clustering {.smaller}

::: incremental
-   Clustering is the act of partitioning data into meaningful groups based on similarity of attributes

-   The goal of clustering is to create insightful clusters to better understand connections in the data
:::

## Clustering {.smaller}

::: columns
::: {.column width="50%"}
::: fragment
-   $k=4$, the seed points are green, forming 4 respective clusters
-   Once the initial assignment of centroids is made, the Euclidean distance is used to establish cluster members by minimizing distance (red lines in the figure)
:::
:::

::: {.column width="50%"}
::: fragment
![centroids](centroids.png)
:::
:::
:::

::: notes
This example the data are clustered based on 4 randomly set seed centers, k=4.
:::

## Euclidean Distance {.smaller}

::: columns
::: {.column width="60%"}
::: fragment
-   Euclidean distance is used to find dissimilarity between data points in order to group similar data instances into clusters
-   In k-means it is done so by calculating the distance between a data object and cluster center using the following formula:
:::

::: fragment
$$d(x,C_i)=sqrt(\sum_{i=1}^{N} (x_jâˆ’C_{ij})^2)$$
:::
:::

::: {.column width="40%"}
::: fragment
Objective Function:

-   The k-means clustering objective is to minimize the within-cluster sum of squares (variance)
:::

::: fragment
It is formulated as:

$$ d(x,C_i)=(\sum_{i=1}^{k}*\sum_{x \in C_i}^{}(||x-\mu_i||)^2) $$
:::
:::
:::

## Euclidean Distance {.smaller}

::: nonincremental
-   $k$ is the number of clusters

-   $C_i$ represents the number of points in the cluster $i$

-   $\mu_i$ represents the centroid mean of cluster $i$

-   In this context, similarity is inversely related to the Euclidean distance

-   The smaller the distance, the greater the similarity between objects
:::

## New Centroids {.smaller}

::: nonincremental
::: columns
::: {.column width="50%"}
-   K-means clustering reassigns the data points to each cluster based on the Euclidean Distance calculation

-   A new centroid location is set by updating the position of each clusters mean center
:::

::: {.column width="50%"}
![data points reassigned to centroids](clusters.png)
:::
:::
:::

## Determining K {.smaller}

::: incremental
-   The elbow method plots the within cluster sum of squares (WCSS) by $k$
:::

::: fragment
```{r}
#| echo: fenced
# Create an empty vector to store WCSS values
wcss <- vector("numeric", length = 10)
# Iterate over a range of K values (e.g., from 1 to 10)
for (i in 1:10) {
  model <- kmeans(df_norm[c("Sales", "Orders", "AvgSale")], centers = i, nstart = 10)
  wcss[i] <- ceiling(model$tot.withinss)
}
```
:::

::: notes
-   WCSS notates the within cluster sums of squares total

-   While cluster numbers are selected before the analysis, the cluster location in the data is randomly generated as mentioned
:::

## Elbow Method {.smaller}

::: panel-tabset
### Code

```{r, `code-line-numbers` "6-11"}
#| echo: true
#| fig-width: 10
#| fig-height: 4.5
library(ggplot2)
# Plot the WCSS values against the number of clusters
p1<- ggplot(data.frame(K=1:10, WCSS=wcss), aes(x=K, y=WCSS)) +
  geom_line() +
  geom_point() +
  labs(title="Elbow Method to Find Optimal K", x="Number of Clusters (K)", y="Within-Cluster-Sum-of-Squares (WCSS)") +
  scale_x_continuous(breaks = seq(0, 10, by = 1))
```

### Plot {.smaller}

```{r}
p1
```
:::

::: notes
Based on the results of the Elbow Method, above, the total WCSS appears to decrease less and less starting at 4. This gives us the ideal starting value for the initial number of clusters in the K-means model.

To understand the model results we compare the results in the table for each cluster, considering what each value tell us about the customer interactions and how they effect the retailer.
:::

## Clustering Model & Results:

```{r}
#| echo: true
set.seed(100)
model1 <- kmeans(df_norm[c("Sales", "Orders", "AvgSale")],4)
tidy(model1)
```

## Cluster Interpretation

::: nonincremental
Cluster 1:

-   Characteristics: Lower total sales, low orders, average per-order sales
-   Recommendation: Targeted marketing for newer customers

Cluster 2:

-   Characteristics: Average total sales, below-average orders, very high per-order sales
-   Recommendation: Marketing high-value items to infrequent buyers
:::

## Cluster Interpretation

::: nonincremental
Cluster 3:

-   Characteristics: Higher total sales, higher orders, average per-order value
-   Recommendation: Recommend low to mid-priced items for frequent buyers

Cluster 4:

-   Characteristics: Lowest total sales, low orders, low per-order sales
-   Recommendation: Minimal marketing efforts due to low ROI
:::

## Cluster Interpretation

```{r, echo=TRUE, error=FALSE, warning=FALSE, message=FALSE}
#| label: model-charts
#| fig-cap: "*Interactive graph (left-click to rotate / right-click to move)*"
plot_ly(customer_data2, x=~Orders, y=~Sales, z=~AvgSale,
        color=~Cluster2,
        colors=c("red","green","blue","violet")) %>% 
  add_markers(size=2) %>% 
  layout(scene = list(xaxis=axx,yaxis=axy,zaxis=axz))
```

## Future Works

::: nonincremental
Challenges and Considerations -

-   Data Handling:

    Managing large and noisy datasets

-   Robustness:

    Ensuring robustness against outliers

-   Cluster Number Determination:

    Defining an appropriate number of clusters
:::

## Future Works

::: nonincremental
Research Focus -

-   Continued Exploration:

    Ongoing refinement of clustering techniques and cluster selection process

-   Industry Evolution:

    Adapting newer methods to meet evolving e-commerce demands
:::
