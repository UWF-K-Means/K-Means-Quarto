% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={K-Means Clustering And Its Place in Ecommerce},
  pdfauthor={Addison Armstrong, Alesha Bolin, Chandan Reddy Yapala},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{K-Means Clustering And Its Place in Ecommerce}
\author{Addison Armstrong, Alesha Bolin, Chandan Reddy Yapala}
\date{9/27/23}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, enhanced, breakable, interior hidden, boxrule=0pt, sharp corners, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{cover}{%
\chapter{Cover}\label{cover}}

\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

Platforms for e-commerce gather gigabytes, terabytes, and even
petabytes, of information about consumer interactions, product
preferences, and transactional histories. There is a heightened demand
for more robust analytical processes as we see data growing at an
unprecedented rate. The use of unsupervised machine learning and
efficient clustering algorithms is crucial to the marketing and
e-commerce industry when it comes to delivering meaningful and accurate
communications to consumers. If properly tapped into, this abundance of
data has the potential to fundamentally alter how internet businesses
personalize and enhance their services and growth strategies. In this
setting, K-Means clustering, a cornerstone of data science, emerges as a
guiding light, promising to uncover the hidden structures in e-commerce
information and offer a deeper comprehension of consumer behavior. The
volume of data will continue to grow as more people have access to an
ever expanding internet for social media, online shopping, and even
remote working opportunities, and so will the need for efficient and
effective consumer marketing methods continue to grow.

``Clustering has proven efficient in discovering subtle but tactical
patterns or relationships buried within a repository of unlabeled data
sets'' (Ali and Kadhum 2017).The k-means clustering method takes care of
the disadvantages seen in most other clustering classifications and has
the ability to handle large amounts of data efficiently while
considering many different attributes. Clustering techniques are mostly
useful in exploratory data analysis and data mining however the k-means
clustering is particularly known for its simplicity, and low
computational complexity and seen as the most popular algorithm suitable
in various applications.

Unsupervised machine learning methods are powerful statistical modeling
techniques that allow for data to be categorized or grouped without the
assistance of historical results or human intervention. One of the
longer-standing methods of autonomous data categorization comes in the
form of the K-Means Clustering algorithm, and is generally selected for
its simplicity and speed (Å½alik 2008). The K-means method is considered
unsupervised machine learning because the outcome is rather data driven
, which is preferred as the formulation can be adjusted based on any
changing dimensions in data.

The basic method of the k-means algorithm is to randomly identify a
centroid for every cluster as k, and use it as a reference point to
assign all data points that retain the greatest similarity to that
center, creating a cluster. The Euclidean distance considers the maximum
distance between cluster means and each data object; it is used to
identify the cluster each object best belongs to. This is a unique
application because the centroid is updated every iteration by
calculating the new mean with those new cluster members, nearly
perfecting the requirement for within cluster similarities. ``The
k-means clustering method is considered a variance minimization
technique. It represents the data as a mixture of distributions
(Gaussian, Poisson, etc.), where each distribution represents a
sub-population (or cluster) of the data''(Morissette 2013). Our
examination provides a thorough comprehension of its mathematical
foundations and clustering concepts. We will show how K-Means may be
used to classify consumer bases into discrete clusters based on shared
characteristics in e-commerce data sets. Additionally, we will discuss
what can be considered drawbacks of the k-means clustering method such
as needing to predetermine the cluster number prior to analysis.

K-Means Clustering helps organizations better target their marketing
efforts and increase return on investment by defining client segments in
the context of marketing. For space-efficient picture compression, image
processing uses K-Means, which is important for web graphics and mobile
applications. K-Means excels in anomaly detection in fraud detection,
protecting financial systems and networks. Additionally, it is used by
recommendation engines to customize the distribution of content,
increasing user happiness and engagement. K-Means promotes patient
profiling in the healthcare industry, advancing individualized
treatments and medication development. Natural language processing uses
its strength to group text data, improving content categorization and
information retrieval. K-Means is a tool for geographic data analysis
that provides regional understanding for urban planning and
environmental studies.

This paper will explore how K-Means clustering is used in real-world
e-commerce scenarios. This paper also highlights the practical
advantages that e-commerce enterprises can gain from applying K-Means
clustering techniques using real-world data.

This study examines K-Means clustering's promise as well as the
complexities and limitations of doing so on e-commerce data sets.
Managing large and noisy data sets, guaranteeing robustness against
outliers, and determining an appropriate value for the ``K'' parameter
are a few of these difficulties that may need to be overcome. Our
investigation will include methods for resolving these issues and
enhancing the robustness and dependability of K-Means clustering as a
tool for e-commerce analytics.

\bookmarksetup{startatroot}

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

The K-Means method, as described by Steinley, is designed to partition
data that is \(N\) objects having measurements on \(P\) variables, into
\(K\) classes (\(C_1\),\(C_2\),\(C_3\),\ldots,\(C_k\)), where \(C_k\) is
the set of \(n_k\) objects in cluster \(k\), where \(K\) is given
(Steinley 2006) The method computes seed points, which are randomly
assigned reference points called centroids. Taking note that a centroid
is not considered a physically observed data point, but rather a
location in the center of a three dimensional space that is used as a
reference point for a cluster. The distance between the centroids and
the surrounding data points is then calculated using a distance formula.
Data points are measured and assigned to the closest centroid. Then the
mean is derived for each newly created cluster which becomes the new
centroid of each cluster. The process is then repeated until there are
no changes in the distance and mean of data points to the assigned
cluster centroids. While there are different distance calculations to
choose from, like the Manhattan Distance or Chebyshev Distance, the most
commonly used is the Euclidean Distance.

The k-Means algorithm relies on Euclidean distance as a measure of
similarity between data objects.Euclidean distance is a powerful tool
for measuring similarity in various applications, including image
processing, pattern recognition, and clustering, such as in the k-means
algorithm, where it helps partition data into well-defined clusters
based on their spatial relationships in the feature space.Euclidean
distance can be employed as a key - metric to measure the dissimilarity
between data points, facilitating the grouping of similar data instances
into clusters using the k-means clustering algorithm. This allows us to
uncover patterns and relationships within the defined set of data, which
is critical for achieving the research objectives.

In this context, similarity is inversely related to the Euclidean
distance; in other words, the smaller the distance, the greater the
similarity between objects. To initiate the algorithm, you must specify
the initial number of clusters \((K)\) and their initial centers. The
algorithm then iteratively adjusts these cluster centers based on the
similarity between data objects and cluster centers. Clustering
continues until the objective function converges, signaling the end of
the process and yielding the final result.

The formula for calculating the Euclidean distance between a data object
and a cluster center can be expressed as follows:

\[
d(x,C_i)=sqrt(\sum_{i=1}^{N} (x_jâC_{ij})^2)
\]\\
Objective Function: The K-means clustering objective is to minimize the
within-cluster variance. It is formulated as

\[
d(x,C_i)=(\sum_{i=1}^{k}*\sum_{x \in C_i}^{}(||x-\mu_i||)^2)
\]

Where:\\
\(k\) is the number of clusters.

\(C_i\) represents the number of points in the cluster \(i\)

\(\mu_i\) represents the centroid mean of cluster \(i\).

The objective function is the sum of squared Euclidean distances from
each point in a cluster to its centroid. The goal is to find cluster
assignments and centroids that minimize this objective.

To determine k, the number of clusters to initially set, we can use the
empirical method or the elbow method. The empirical method assigns the
number of clusters \(\approx \frac{n^{\frac{1}{2}}}{2}\) for a data set
of \(n\) observations. The elbow method is based on the idea that
increasing the number of clusters k will reduce the sum of
within-cluster variance. After plotting the sum of within-cluster
variances as a function of the number of clusters \(K\), the inflection
point is determined using the turning point in the curve.

The k-means algorithm for partitioning can be executed in 5 main steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Partition \(N\) objects, or observations, into \(K\) nonempty subsets.
  Each cluster must not be empty and will have a different
  classification. This is \(K\), if \(k=4\) that means there will be two
  clusters.
\item
  Then compute seed points, which are randomly assigned reference points
  called centroids. In Figure~\ref{fig-centroids} \(k=4\) and are
  highlighted in green.
\item
  Assign each object to the cluster with the nearest assigned centroid.
  Figure~\ref{fig-centroids} shows the Euclidean distance calculation as
  a line connecting each observation to one of the 4 centroids.
\end{enumerate}

\begin{figure}

{\centering \includegraphics{./centroids.png}

}

\caption{\label{fig-centroids}centroids}

\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Adjust the centroid location using the Euclidean distance formula to
  minimize the distance of each data observation to its assigned
  centroid. Figure~\ref{fig-clusters} shows how the Euclidean distance
  drives some cluster members to reassign to a new cluster, influencing
  the centroids positions. This updates the cluster mean ,\(\mu_i\), in
  turn relocating the centroids from where they were seen in
  (\textbf{figure-centroids?}).
\item
  The final step is to repeat steps 2-4 until there is no change in
  distance between the observations and the centroids.
\end{enumerate}

\begin{figure}

{\centering \includegraphics{./clusters.png}

}

\caption{\label{fig-clusters}clusters}

\end{figure}

The result uncovers 4 distinct clusters containing the most similar
observations within groups.

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Huda2017}{}}%
Ali, Huda H., and Lubna Emad Kadhum. 2017. {``K- Means Clustering
Algorithm Applications in Data Mining and Pattern Recognition.''} In.
\url{https://api.semanticscholar.org/CorpusID:36213323}.

\leavevmode\vadjust pre{\hypertarget{ref-Morissette2013}{}}%
Morissette, Sylvain, Laurence AND Chartier. 2013. {``The k-Means
Clustering Technique: General Considerations and Implementation in
Mathematica.''} \emph{Tutorials in Quantitative Methods for Psychology}
9 (1): 15--24. \url{https://doi.org/10.20982/tqmp.09.1.p015}.

\leavevmode\vadjust pre{\hypertarget{ref-Steinley2006}{}}%
Steinley, Douglas. 2006. {``K-Means Clustering: A Half-Century
Synthesis.''} \emph{British Journal of Mathematical \& Statistical
Psychology} 59 (May): 1--34.
\url{https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/k-means-clustering-half-century-synthesis/docview/216465055/se-2}.

\leavevmode\vadjust pre{\hypertarget{ref-Zalik2008}{}}%
Å½alik, Krista Rizman. 2008. {``An Efficient kâ²-Means Clustering
Algorithm.''} \emph{Pattern Recognition Letters} 29 (9): 1385--91.
https://doi.org/\url{https://doi.org/10.1016/j.patrec.2008.02.014}.

\end{CSLReferences}



\end{document}
