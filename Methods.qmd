# Overview {.unnumbered}

---
title: "Overview"
output: html_document
---

The k-means method, as described by Steinley, is designed to partition data that is $N$ objects having measurements on $P$ variables, into $K$ classes ($C_1$,$C_2$,$C_3$,...,$C_k$), where $C_k$ is the set of $n_k$ objects in cluster $k$, where $K$ is given [@Steinley2006]. The method computes seed points, which are randomly assigned reference points called centroids. Taking note that a centroid is not considered a physically observed data point, but rather a location in the center of a three dimensional space that is used as a reference point for a cluster. The distance between the centroids and the surrounding data points is then calculated using a distance formula. Data points are measured and assigned to the closest centroid. Then the mean is derived for each newly created cluster which becomes the new centroid of each cluster. The process is then repeated until there are no changes in the distance and mean of data points to the assigned cluster centroids. While there are different distance calculations to choose from, like the Manhattan Distance or Chebyshev Distance, the most commonly used is the Euclidean Distance.

The k-means algorithm relies on Euclidean distance as a measure of similarity between data objects. Euclidean distance is a powerful tool for measuring similarity in various applications, including image processing, pattern recognition, and clustering, such as in the k-means algorithm, where it helps partition data into well-defined clusters based on their spatial relationships in the feature space. Euclidean distance can be employed as a key metric to measure the dissimilarity between data points, facilitating the grouping of similar data instances into clusters using the k-means clustering algorithm. This allows us to uncover patterns and relationships within the defined set of data, which is critical for achieving the research objectives.

In this context, similarity is inversely related to the Euclidean distance; in other words, the smaller the distance, the greater the similarity between objects. To initiate the algorithm, an initial number of clusters $(K)$ and their initial centers are selected. The algorithm then iteratively adjusts these cluster centers based on the similarity between data objects and cluster centers. Clustering continues until the objective function converges, signaling the end of the process and yielding the final result.
